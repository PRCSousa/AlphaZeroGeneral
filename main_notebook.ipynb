{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Zero For Generalized Game Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pygame\n",
    "import pickle\n",
    "import socket\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import trange\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go\n",
    "\n",
    "Go is an ancient board game that is believed to be originated in China over 4000 years ago. It's usually played on a 19x19 grid board by two players, one using black stones and the other white stones. The game is known for its simple rules but depth in strategy. The primary objective in Go is to control more territory on the board than your opponent and this is gained by surrounding empty areas of the board with your stones. Players also aim to capture their opponent's stones by completely surrounding them.\n",
    "\n",
    "### Game Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Go():\n",
    "\n",
    "    EMPTY = 0\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "    BLACKMARKER = 4\n",
    "    WHITEMARKER = 5\n",
    "    LIBERTY = 8\n",
    "\n",
    "    def __init__(self, size, komi):\n",
    "        self.row_count = size\n",
    "        self.column_count = size\n",
    "        self.komi = 5.5\n",
    "        self.action_size = self.row_count * self.column_count + 1\n",
    "        self.liberties = []\n",
    "        self.block = []\n",
    "        self.seki_liberties = []\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns a board of the argument size filled of zeros.\n",
    "\n",
    "        # Retuns:\n",
    "        Empty board full of zeros\n",
    "        '''\n",
    "        board = np.zeros((self.row_count, self.column_count))\n",
    "        return board\n",
    "    \n",
    "\n",
    "    def count(self, x, y, state: list, player:int , liberties: list, block: list) -> tuple[list, list]:\n",
    "        '''\n",
    "        # Description:\n",
    "        Counts the number of liberties of a stone and the number of stones in a block.\n",
    "        Follows a recursive approach to count the liberties of a stone and the number of stones in a block.\n",
    "\n",
    "        # Returns:\n",
    "        A tuple containing the number of liberties and the number of stones in a block.\n",
    "        '''\n",
    "        \n",
    "        #initialize piece\n",
    "        piece = state[y][x]\n",
    "        #if there's a stone at square of the given player\n",
    "        if piece == player:\n",
    "            #save stone coords\n",
    "            block.append((y,x))\n",
    "            #mark the stone\n",
    "            if player == self.BLACK:\n",
    "                state[y][x] = self.BLACKMARKER\n",
    "            else:\n",
    "                state[y][x] = self.WHITEMARKER\n",
    "            \n",
    "            #look for neighbours recursively\n",
    "            if y-1 >= 0:\n",
    "                liberties, block = self.count(x,y-1,state,player,liberties, block) #walk north\n",
    "            if x+1 < self.column_count:\n",
    "                liberties, block = self.count(x+1,y,state,player,liberties, block) #walk east\n",
    "            if y+1 < self.row_count:\n",
    "                liberties, block = self.count(x,y+1,state,player,liberties, block) #walk south\n",
    "            if x-1 >= 0:\n",
    "                liberties, block = self.count(x-1,y,state,player,liberties, block) #walk west\n",
    "\n",
    "        #if square is empty\n",
    "        elif piece == self.EMPTY:\n",
    "            #mark liberty\n",
    "            state[y][x] = self.LIBERTY\n",
    "            #save liberties\n",
    "            liberties.append((y,x))\n",
    "\n",
    "        # print(\"Liberties: \" + str(len(self.liberties)) + \" in: \" + str(x) + \",\" + str(y))\n",
    "        # print(\"Block: \" + str(len(self.block)) + \" in: \" + str(x) + \",\" + str(y))\n",
    "        return liberties, block\n",
    "\n",
    "    #remove captured stones\n",
    "    def clear_block(self, block: list, state: list) -> list:\n",
    "        '''\n",
    "        # Description:\n",
    "        Clears the block of stones captured by the opponent on the board.\n",
    "\n",
    "        # Returns:\n",
    "        The board with the captured stones removed.\n",
    "        '''\n",
    "\n",
    "        #clears the elements in the block of elements which is captured\n",
    "        for i in range(len(block)): \n",
    "            y, x = block[i]\n",
    "            state[y][x] = self.EMPTY\n",
    "        \n",
    "        return state\n",
    "\n",
    "    #restore board after counting stones and liberties\n",
    "    def restore_board(self, state: list) -> list:\n",
    "        '''\n",
    "        # Description:\n",
    "        Restores the board to its original state after counting liberties and stones.\n",
    "        This is done by unmarking the stones following bitwise operations with the global class variables.\n",
    "        \n",
    "        # Returns:\n",
    "        The board with the stones unmarked.\n",
    "        '''\n",
    "\n",
    "        #unmark stones\n",
    "        # print(\"Restore Board\")\n",
    "        # print(state)\n",
    "        for y in range(len(state)):\n",
    "            for x in range(len(state)):\n",
    "                #restore piece\n",
    "                val = state[y][x]\n",
    "                if val == self.BLACKMARKER:\n",
    "                    state[y][x] = self.BLACK\n",
    "                elif val == self.WHITEMARKER:\n",
    "                    state[y][x] = self.WHITE\n",
    "                elif val == self.LIBERTY:\n",
    "                    state[y][x] = self.EMPTY\n",
    "\n",
    "        # print(\"After Restore Board\")\n",
    "        # print(state)\n",
    "        return state\n",
    "\n",
    "    def print_board(self, state: list) -> None:\n",
    "            '''\n",
    "            # Description:\n",
    "            Draws the board in the console.\n",
    "\n",
    "            # Returns:\n",
    "            None\n",
    "            '''\n",
    "\n",
    "        # Print column coordinates\n",
    "            print(\"   \", end=\"\")\n",
    "            for j in range(len(state[0])):\n",
    "                print(f\"{j:2}\", end=\" \")\n",
    "            print(\"\\n  +\", end=\"\")\n",
    "            for _ in range(len(state[0])):\n",
    "                print(\"---\", end=\"\")\n",
    "            print()\n",
    "\n",
    "            # Print rows with row coordinates\n",
    "            for i in range(len(state)):\n",
    "                print(f\"{i:2}|\", end=\" \")\n",
    "                for j in range(len(state[0])):\n",
    "                    print(f\"{str(int(state[i][j])):2}\", end=\" \")\n",
    "                print()\n",
    "    \n",
    "    def captures(self, state: list,player: int, a:int, b:int) -> tuple[bool, list]:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks if a move causes a capture of stones of the player passed as an argument.\n",
    "        If a move causes a capture, the stones are removed from the board.\n",
    "\n",
    "        # Returns:\n",
    "        A tuple containing a boolean indicating if a capture has been made and the board with the captured stones removed.\n",
    "        '''\n",
    "        check = False\n",
    "        neighbours = []\n",
    "        if(a > 0): neighbours.append((a-1, b))\n",
    "        if(a < self.column_count - 1): neighbours.append((a+1, b))\n",
    "        if(b > 0): neighbours.append((a, b - 1))\n",
    "        if(b < self.row_count - 1): neighbours.append((a, b+1))\n",
    "\n",
    "        #loop over the board squares\n",
    "        for pos in neighbours:\n",
    "            # print(pos)\n",
    "            x = pos[0]\n",
    "            y = pos[1]    \n",
    "            # init piece\n",
    "            piece = state[x][y]\n",
    "\n",
    "                #if stone belongs to given colour\n",
    "            if piece == player:\n",
    "                # print(\"opponent piece\")\n",
    "                # count liberties\n",
    "                liberties = []\n",
    "                block = []\n",
    "                liberties, block = self.count(y, x, state, player, liberties, block)\n",
    "                # print(\"Liberties in count: \" + str(len(liberties)))\n",
    "                # if no liberties remove the stones\n",
    "                if len(liberties) == 0: \n",
    "                    #clear block\n",
    "                    state = self.clear_block(block, state)\n",
    "                    check = True\n",
    "\n",
    "                #restore the board\n",
    "                state = self.restore_board(state)\n",
    "\n",
    "        #print(\"Captures: \" + str(check))\n",
    "        return check, state\n",
    "    \n",
    "    def set_stone(self, a, b, state, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Places the piece on the board. THIS DOES NOT account for the rules of the game, use get_next_state().\n",
    "\n",
    "        # Retuns:\n",
    "        Board with the piece placed.\n",
    "        '''\n",
    "        state[a][b] = player\n",
    "        return state\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        '''\n",
    "        # Description\n",
    "        Plays the move, verifies and undergoes captures and saves the state to the history.\n",
    "        \n",
    "        # Returns:\n",
    "        New state with everything updated.\n",
    "        '''\n",
    "        if action == self.row_count * self.column_count:\n",
    "            return state # pass move\n",
    "\n",
    "        a = action // self.row_count\n",
    "        b = action % self.column_count\n",
    "\n",
    "        # checking if the move is part of is the secondary move to a ko fight\n",
    "        state = self.set_stone(a, b, state, player)\n",
    "        # print(state)\n",
    "        state = self.captures(state, -player, a, b)[1]\n",
    "        return state\n",
    "    \n",
    "    def is_valid_move(self, state: list, action: tuple, player: int) -> bool:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks if a move is valid.\n",
    "        If a move repeats a previous state or commits suicide (gets captured without capturing back), it is not valid.\n",
    "        \n",
    "        A print will follow explaining the invalid move in case it exists.\n",
    "\n",
    "        # Returns:\n",
    "        A boolean confirming the validity of the move.\n",
    "        '''\n",
    "\n",
    "        a = action[0]\n",
    "        b = action[1]\n",
    "\n",
    "        #print(f\"{a} , {b}\")\n",
    "\n",
    "        statecopy = np.copy(state).astype(np.int8)\n",
    "\n",
    "        if state[a][b] != self.EMPTY:\n",
    "            # print(\"Space Occupied\")\n",
    "            return False \n",
    "\n",
    "\n",
    "        statecopy = self.set_stone(a,b,statecopy,player)\n",
    "\n",
    "        if self.captures(statecopy, -player, a, b)[0] == True:\n",
    "            return True\n",
    "        else:\n",
    "            #print(\"no captures\")\n",
    "            libs, block = self.count(b,a,statecopy,player,[],[])\n",
    "            #print(libs)\n",
    "            if len(libs) == 0:\n",
    "                #print(\"Invalid, Suicide\")\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        \n",
    "\n",
    "    def get_valid_moves(self, state, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns a matrix with the valid moves for the current player.\n",
    "        '''\n",
    "        newstate = np.zeros((self.row_count, self.column_count))\n",
    "        for a in range(0, self.column_count):\n",
    "            for b in range(0, self.row_count):\n",
    "                if self.is_valid_move(state, (a,b), player):\n",
    "                    newstate[a][b] = 1\n",
    "        \n",
    "        newstate = newstate.reshape(-1)\n",
    "\n",
    "        empty = 0\n",
    "        endgame = True\n",
    "        \n",
    "        for x in range(self.column_count):\n",
    "            for y in range(self.row_count):\n",
    "                if state[x][y] == self.EMPTY:\n",
    "                    empty += 1\n",
    "                    if empty >= self.column_count * self.row_count // 3: # if 2/3ds are already filled, skipping becomes available\n",
    "                        endgame = False\n",
    "                        break\n",
    "        if endgame:\n",
    "            newstate = np.concatenate([newstate, [1]])\n",
    "        else:\n",
    "            newstate = np.concatenate([newstate, [0]])\n",
    "        return (newstate).astype(np.int8)\n",
    "\n",
    "    def get_value_and_terminated(self, state, action, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns the value of the state and if the game is over.\n",
    "        '''\n",
    "\n",
    "        scoring, endgame = self.scoring(state)\n",
    "\n",
    "        if endgame:\n",
    "            if player == self.BLACK:\n",
    "                if scoring > 0:\n",
    "                    return 1, True\n",
    "                else:\n",
    "                    return -1, True\n",
    "            else:\n",
    "                if scoring < 0:\n",
    "                    return 1, True\n",
    "                else:\n",
    "                    return -1, True\n",
    "        else:\n",
    "            if player == self.BLACK:\n",
    "                if scoring > 0:\n",
    "                    return 1, False\n",
    "                else:\n",
    "                    return -1, False\n",
    "            else:\n",
    "                if scoring < 0:\n",
    "                    return 1, False\n",
    "                else:\n",
    "                    return -1, False\n",
    "\n",
    "\n",
    "        \n",
    "    def scoring(self, state: list) -> int:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks the score of the game. Score is calculated using:\n",
    "\n",
    "        black - (white + komi)\n",
    "\n",
    "        # Returns:\n",
    "        Integer with score.\n",
    "        '''\n",
    "        black = 0\n",
    "        white = 0\n",
    "        empty = 0\n",
    "        endgame = True\n",
    "\n",
    "        for x in range(self.column_count):\n",
    "            for y in range(self.row_count):\n",
    "                if state[x][y] == self.EMPTY:\n",
    "                    empty += 1\n",
    "                    if empty >= self.column_count * self.row_count // 4:\n",
    "                        endgame = False\n",
    "                        break\n",
    "\n",
    "        black, white = self.count_influenced_territory_enhanced(state)\n",
    "        black_eyes, black_strong_groups = self.count_eyes_and_strong_groups(state, self.BLACK)\n",
    "        white_eyes, white_strong_groups = self.count_eyes_and_strong_groups(state, self.WHITE)\n",
    "        # print(f\"Black | Territory: {black} Eyes: {black_eyes} Strong Groups: {black_strong_groups}\")\n",
    "        # print(f\"White | Territory: {white} Eyes: {white_eyes} Strong Groups: {white_strong_groups}\")\n",
    "        \n",
    "        black += black_eyes + black_strong_groups\n",
    "        white += white_eyes + white_strong_groups\n",
    "        \n",
    "        return black - (white + self.komi), endgame\n",
    "    \n",
    "    def count_influenced_territory_enhanced(self, board: list) -> tuple[int, int]:\n",
    "        '''\n",
    "        # Description \n",
    "        Calculates the territory influenced by black and white players on the Go board.\n",
    "\n",
    "        This function iterates through the board, analyzing each empty point to determine \n",
    "        if it's influenced by the surrounding black or white stones. The influence is calculated\n",
    "        based on the adjacent stones, with positive scores indicating black influence and negative\n",
    "        scores indicating white influence.\n",
    "\n",
    "        # Returns:\n",
    "        Tuple (black_territory, white_territory)\n",
    "        '''\n",
    "        black_territory = 0\n",
    "        white_territory = 0\n",
    "        visited = set()\n",
    "\n",
    "        # Function to calculate influence score\n",
    "        def influence_score(x, y):\n",
    "            score = 0\n",
    "            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < len(board) and 0 <= ny < len(board[0]):\n",
    "                    score += board[nx][ny]\n",
    "            return score\n",
    "\n",
    "        # Function to explore territory\n",
    "        def explore_territory(x, y):\n",
    "            nonlocal black_territory, white_territory\n",
    "            if (x, y) in visited or not (0 <= x < len(board) and 0 <= y < len(board[0])):\n",
    "                return\n",
    "            visited.add((x, y))\n",
    "\n",
    "            if board[x][y] == 0:\n",
    "                score = influence_score(x, y)\n",
    "                if score > 0:\n",
    "                    black_territory += 1\n",
    "                elif score < 0:\n",
    "                    white_territory += 1\n",
    "\n",
    "        for i in range(len(board)):\n",
    "            for j in range(len(board[0])):\n",
    "                if board[i][j] == 0 and (i, j) not in visited:\n",
    "                    explore_territory(i, j)\n",
    "\n",
    "        return black_territory, white_territory\n",
    "    \n",
    "    def is_eye(self, board, x, y, player):\n",
    "\n",
    "        # An eye is an empty point with all adjacent points of the player's color\n",
    "        # and at least one diagonal point of the player's color.\n",
    "        \n",
    "        if board[x][y] != self.EMPTY:\n",
    "            return False\n",
    "        \n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if not (0 <= nx < len(board) and 0 <= ny < len(board[0])):\n",
    "                continue\n",
    "            if board[nx][ny] != player:\n",
    "                return False\n",
    "            \n",
    "        true_eye = False\n",
    "        count = 0\n",
    "        for dx, dy in [(1, 1), (1, -1), (-1, 1), (-1, -1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "\n",
    "            if 0 <= nx < len(board) and 0 <= ny < len(board[0]) and board[nx][ny] == player:\n",
    "                count += 1\n",
    "                if count >= 3:\n",
    "                    true_eye = True\n",
    "\n",
    "\n",
    "        return true_eye\n",
    "\n",
    "    def count_eyes_and_strong_groups(self, board, player):\n",
    "        eyes = 0\n",
    "        strong_groups = 0\n",
    "        visited = set()\n",
    "\n",
    "        def dfs(x, y):\n",
    "            if (x, y) in visited or board[x][y] != player:\n",
    "                return 0\n",
    "\n",
    "            visited.add((x, y))\n",
    "            liberties = 0\n",
    "            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if not (0 <= nx < len(board) and 0 <= ny < len(board[0])):\n",
    "                    continue\n",
    "                if board[nx][ny] == self.EMPTY:\n",
    "                    liberties += 1\n",
    "                elif board[nx][ny] == player:\n",
    "                    liberties += dfs(nx, ny)\n",
    "\n",
    "            return liberties\n",
    "\n",
    "        for x in range(len(board)):\n",
    "            for y in range(len(board[0])):\n",
    "                if board[x][y] == player and (x, y) not in visited:\n",
    "                    liberties = dfs(x, y)\n",
    "                    if liberties >= 2:  # Arbitrary threshold for a strong group\n",
    "                        strong_groups += 1\n",
    "                if board[x][y] != player and (x, y) not in visited and self.is_eye(board, x, y, player):\n",
    "                    eyes += 1\n",
    "\n",
    "        return eyes, strong_groups\n",
    "\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Changes Opponent\n",
    "        '''\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        '''\n",
    "        # Description\n",
    "        Returns the negative value of the value\n",
    "        '''\n",
    "        return -value\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        '''\n",
    "        # Description: \n",
    "        Converts the current state of the Go board into a 3-layer encoded format suitable for neural network input.\n",
    "        Each layer in the encoded format represents the presence of a specific type of stone or an empty space on the board:\n",
    "        - Layer 1 encodes the positions of white stones (represented by -1 in the input state) as 1s, and all other positions as 0s.\n",
    "        - Layer 2 encodes the positions of empty spaces (represented by 0 in the input state) as 1s, and all other positions as 0s.\n",
    "        - Layer 3 encodes the positions of black stones (represented by 1 in the input state) as 1s, and all other positions as 0s.\n",
    "        This encoding helps in clearly distinguishing between different elements on the board for machine learning applications.\n",
    "\n",
    "        # Returns: \n",
    "        A NumPy array of shape (3, height, width) containing the 3-layer encoded representation of the board state. Each layer is a 2D array where the board's height and width correspond to the dimensions of the original state.\n",
    "        '''\n",
    "        layer_1 = np.where(np.array(state) == -1, 1, 0).astype(np.float32)\n",
    "        layer_2 = np.where(np.array(state) == 0, 1, 0).astype(np.float32)\n",
    "        layer_3 = np.where(np.array(state) == 1, 1, 0).astype(np.float32)\n",
    "\n",
    "        result = np.stack([layer_1, layer_2, layer_3]).astype(np.float32)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        '''\n",
    "        # Description: \n",
    "        Adjusts the perspective of the Go board state based on the current player.\n",
    "\n",
    "        # Returns: \n",
    "        A two-dimensional array representing the Go board state adjusted for the current player's perspective.\n",
    "        '''\n",
    "        return state * player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Interface Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_BOARD = 9\n",
    "BLACK = (0,0,0)\n",
    "WHITE = (255,255,255)\n",
    "WOOD = (216, 166, 91)\n",
    "SCREEN_SIZE = 600\n",
    "SCREEN_PADDING = 50\n",
    "CELL_SIZE = (SCREEN_SIZE - SCREEN_PADDING) // SIZE_BOARD\n",
    "PIECE_SIZE = (SCREEN_SIZE - 2*SCREEN_PADDING) // SIZE_BOARD // 3\n",
    "screen=pygame.display.set_mode((SCREEN_SIZE,SCREEN_SIZE))\n",
    "\n",
    "def goto_pixels(x):\n",
    "    return SCREEN_PADDING + x*CELL_SIZE\n",
    "\n",
    "def goto_coord(x):\n",
    "    quarter = CELL_SIZE//4\n",
    "    closest = (x-SCREEN_PADDING)//CELL_SIZE\n",
    "    if abs(goto_pixels(closest)-(x-SCREEN_PADDING > goto_pixels(closest)-(x-SCREEN_PADDING+quarter))):\n",
    "        closest = (x-SCREEN_PADDING+quarter)//CELL_SIZE\n",
    "    return closest\n",
    "\n",
    "def godraw_board():\n",
    "    pygame.draw.rect(screen, WOOD, rect=(SCREEN_PADDING, SCREEN_PADDING, CELL_SIZE*(SIZE_BOARD-1), CELL_SIZE*(SIZE_BOARD-1)))\n",
    "    for i in range(SIZE_BOARD):\n",
    "        pygame.draw.line(screen, BLACK,(goto_pixels(i),SCREEN_PADDING),(goto_pixels(i),CELL_SIZE*(SIZE_BOARD-1) + SCREEN_PADDING),3)\n",
    "        pygame.draw.line(screen, BLACK,(SCREEN_PADDING,goto_pixels(i)),(CELL_SIZE*(SIZE_BOARD-1)+SCREEN_PADDING,goto_pixels(i)),3)\n",
    "\n",
    "def godraw_piece(x,y,player):\n",
    "    color = BLACK if player == -1 else WHITE\n",
    "    pygame.draw.circle(screen,color,(goto_pixels(x),goto_pixels(y)),PIECE_SIZE)\n",
    "    pygame.draw.circle(screen,BLACK,(goto_pixels(x),goto_pixels(y)),PIECE_SIZE,3)\n",
    "\n",
    "def gohover_to_select(player,valid_moves,click):\n",
    "    mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "    x, y = None, None\n",
    "    if ([goto_coord(mouse_x), goto_coord(mouse_y)] in valid_moves):\n",
    "        x, y = goto_coord(mouse_x), goto_coord(mouse_y)\n",
    "    \n",
    "    if (x!=None):\n",
    "        pixels = (goto_pixels(x),goto_pixels(y))\n",
    "        distance = pygame.math.Vector2(pixels[0] - mouse_x, pixels[1] - mouse_y).length()\n",
    "        if distance < PIECE_SIZE:\n",
    "            s = pygame.Surface((SCREEN_SIZE, SCREEN_SIZE), pygame.SRCALPHA)\n",
    "            if player == 1:\n",
    "                pygame.draw.circle(s,(255,255,255,200),(goto_pixels(x),goto_pixels(y)),PIECE_SIZE)\n",
    "            if player == -1:\n",
    "                pygame.draw.circle(s,(0,0,0,200),(goto_pixels(x),goto_pixels(y)),PIECE_SIZE)\n",
    "            pygame.draw.circle(s,BLACK,(goto_pixels(x),goto_pixels(y)),PIECE_SIZE,3)\n",
    "            screen.blit(s, (0, 0))\n",
    "\n",
    "    return [None, None, player]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaxx\n",
    "\n",
    "Attaxx is a strategy board game that was developed in 1990 by the French company Taito. The game is played on an 7x7 grid, and each player starts with two pieces placed in opposite corners of the board. The objective of Attaxx is to have more pieces on the board than your opponent when the game concludes.\n",
    "\n",
    "### Game Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attaxx:\n",
    "    def __init__(self, args):\n",
    "        self.column_count = args[0]\n",
    "        self.row_count = args[1]\n",
    "        self.action_size = (self.column_count * self.row_count) ** 2 + 1\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        state = np.zeros((self.column_count, self.row_count))\n",
    "        state[0][0] = 1\n",
    "        state[self.column_count-1][self.row_count-1] = 1\n",
    "        state[0][self.column_count-1] = -1\n",
    "        state[self.row_count-1][0] = -1\n",
    "        return state\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        if action == self.action_size - 1:\n",
    "            return state\n",
    "        move = self.int_to_move(action)\n",
    "        a, b, a1, b1 = move[0], move[1], move[2], move[3]\n",
    "        if abs(a-a1)==2 or abs(b-b1)==2:\n",
    "            state[a][b] = 0\n",
    "            state[a1][b1] = player\n",
    "        else:\n",
    "            state[a1][b1] = player\n",
    "        self.capture_pieces(state, move, player)\n",
    "        return state\n",
    "\n",
    "    def is_valid_move(self, state, action, player):\n",
    "        a, b, a1, b1 = action\n",
    "        if (a==a1 and b==b1):\n",
    "            return False\n",
    "        if abs(a-a1)>2 or abs(b-b1)>2 or state[a1][b1]!=0 or state[a][b]!=player or ((abs(a-a1)==1 and abs(b-b1)==2) or (abs(a-a1)==2 and abs(b-b1)==1)):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def capture_pieces(self, state, action, player):\n",
    "        a, b, a1, b1 = action\n",
    "        for i in range(a1-1, a1+2):\n",
    "            for j in range(b1-1, b1+2):\n",
    "                try:\n",
    "                    if state[i][j]==-player and i>=0 and j>=0:\n",
    "                        state[i][j] = player\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "    def check_available_moves(self, state, player):\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                if state[i][j] == player:\n",
    "                    for a in range(self.column_count):\n",
    "                        for b in range(self.row_count):\n",
    "                            action = (i, j, a, b)\n",
    "                            if self.is_valid_move(state, action, player):\n",
    "                                return True\n",
    "        return False\n",
    "\n",
    "    def move_to_int(self, move):\n",
    "        return move[3] + move[2]*self.column_count + move[1]*self.column_count**2 + move[0]*self.column_count**3\n",
    "\n",
    "    def int_to_move(self, num):\n",
    "        move = [(num // self.column_count**3) % self.column_count, \n",
    "                (num // self.column_count**2) % self.column_count, \n",
    "                (num // self.column_count) % self.column_count, \n",
    "                num % self.column_count]\n",
    "        return move\n",
    "\n",
    "    \n",
    "    def get_valid_moves(self, state, player):\n",
    "        possible_moves = set()\n",
    "\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                state[i][j] = int(state[i][j])\n",
    "                if state[i][j] == player:\n",
    "                    moves_at_point = set(self.get_moves_at_point(state, player, i, j))\n",
    "                    possible_moves = possible_moves.union(moves_at_point)\n",
    "        \n",
    "        possible_moves_to_int = []\n",
    "        for move in possible_moves:\n",
    "            possible_moves_to_int.append(self.move_to_int(move))\n",
    "        binary_representation = [1 if i in possible_moves_to_int else 0 for i in range(self.action_size)]\n",
    "\n",
    "        return binary_representation\n",
    "\n",
    "    def get_moves_at_point(self, state, player, a, b):\n",
    "        moves_at_point = []\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                possible_action = (a, b, i, j)\n",
    "                if self.is_valid_move(state, possible_action, player):\n",
    "                    moves_at_point.append(possible_action)\n",
    "        return moves_at_point \n",
    "\n",
    "    def check_board_full(self, state):\n",
    "        for row in state:\n",
    "            if 0 in row:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def check_win_and_over(self, state, action):\n",
    "        # action is not necessary for attaxx, but is necessary for go\n",
    "\n",
    "        count_player1 = 0\n",
    "        count_player2 = 0\n",
    "\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                if state[i][j] == 1:\n",
    "                    count_player1+=1\n",
    "                elif state[i][j] == -1:\n",
    "                    count_player2+=1\n",
    "        if count_player1 == 0:\n",
    "            return -1, True\n",
    "        elif count_player2 == 0:\n",
    "            return 1, True\n",
    "        \n",
    "        if self.check_board_full(state):\n",
    "            if count_player1>count_player2:\n",
    "                return 1, True\n",
    "            elif count_player2>count_player1:\n",
    "                return -1, True\n",
    "            elif count_player1==count_player2:\n",
    "                return 2, True\n",
    "        \n",
    "        return 0, False\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action, player):\n",
    "        winner, game_over = self.check_win_and_over(state, action = None)\n",
    "        return winner, game_over\n",
    "    \n",
    "    def print_board(self, state):\n",
    "        state = state.astype(int)\n",
    "        # Print column coordinates\n",
    "        print(\"   \", end=\"\")\n",
    "        for j in range(len(state[0])):\n",
    "            print(f\"{j:2}\", end=\" \")\n",
    "        print(\"\\n  +\", end=\"\")\n",
    "        for _ in range(len(state[0])):\n",
    "            print(\"---\", end=\"\")\n",
    "        print()\n",
    "\n",
    "        # Print rows with row coordinates\n",
    "        for i in range(len(state)):\n",
    "            print(f\"{i:2}|\", end=\" \")\n",
    "            for j in range(len(state[0])):\n",
    "                print(f\"{str(state[i][j]):2}\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        layer_1 = np.where(np.array(state) == -1, 1, 0).astype(np.float32) #returns same sized board replacing all -1 with 1 and all other positions with 0\n",
    "        layer_2 = np.where(np.array(state) == 0, 1, 0).astype(np.float32) #same logic for each possible number in position (-1, 1, or 0)\n",
    "        layer_3 = np.where(np.array(state) == 1, 1, 0).astype(np.float32)\n",
    "        \n",
    "        result = np.stack([layer_1, layer_2, layer_3]).astype(np.float32) #encoded state\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Interface Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_BOARD = 6\n",
    "RED = (238, 167, 255)\n",
    "BLUE = (113, 175, 255)\n",
    "GRAY = (115, 115, 115)\n",
    "BLACK = (0, 0, 0)\n",
    "SCREEN_SIZE=600\n",
    "SCREEN_PADDING = 100\n",
    "CELL_SIZE = (SCREEN_SIZE - SCREEN_PADDING) // SIZE_BOARD\n",
    "PIECE_SIZE = (SCREEN_SIZE - 2*SCREEN_PADDING) // SIZE_BOARD // 3\n",
    "screen=pygame.display.set_mode((SCREEN_SIZE,SCREEN_SIZE))\n",
    "\n",
    "def atto_pixels(x):\n",
    "    return SCREEN_PADDING + x*CELL_SIZE\n",
    "\n",
    "def atto_coord(x):\n",
    "    quarter = CELL_SIZE//4\n",
    "    closest = (x-SCREEN_PADDING)//CELL_SIZE\n",
    "    if abs(atto_pixels(closest)-(x-SCREEN_PADDING > atto_pixels(closest)-(x-SCREEN_PADDING+quarter))):\n",
    "        closest = (x-SCREEN_PADDING+quarter)//CELL_SIZE\n",
    "    return closest\n",
    "\n",
    "def atdraw_board():\n",
    "    pygame.draw.rect(screen, GRAY, rect=(SCREEN_PADDING, SCREEN_PADDING, CELL_SIZE * (SIZE_BOARD - 1), CELL_SIZE * (SIZE_BOARD - 1)))\n",
    "    for i in range(SIZE_BOARD + 1):\n",
    "        # Draw vertical lines\n",
    "        pygame.draw.line(screen, BLACK,\n",
    "                         (SCREEN_PADDING - (PIECE_SIZE*2) + i * CELL_SIZE, SCREEN_PADDING - PIECE_SIZE*2 ),\n",
    "                         (SCREEN_PADDING - PIECE_SIZE*2 + i * CELL_SIZE, SCREEN_PADDING + CELL_SIZE * (SIZE_BOARD) - PIECE_SIZE*2),\n",
    "                         3)\n",
    "\n",
    "        # Draw horizontal lines\n",
    "        pygame.draw.line(screen, BLACK,\n",
    "                         (SCREEN_PADDING - PIECE_SIZE*2, SCREEN_PADDING - PIECE_SIZE*2+ i * CELL_SIZE),\n",
    "                         (SCREEN_PADDING - PIECE_SIZE*2 + CELL_SIZE * (SIZE_BOARD), SCREEN_PADDING - PIECE_SIZE*2 + i * CELL_SIZE),\n",
    "                         3)\n",
    "\n",
    "\n",
    "\n",
    "def atdraw_piece(x,y,player):\n",
    "    color = RED if player == -1 else BLUE\n",
    "    pygame.draw.circle(screen,color,(atto_pixels(x),atto_pixels(y)),PIECE_SIZE)\n",
    "    pygame.draw.circle(screen,BLACK,(atto_pixels(x),atto_pixels(y)),PIECE_SIZE,3)\n",
    "\n",
    "\n",
    "def athover_to_select(sel_x, sel_y, player, valid_moves, click, selected_piece, cur_pieces, last_click_time):\n",
    "\n",
    "    current_time = pygame.time.get_ticks()\n",
    "    mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "    x, y = None, None\n",
    "    if ([atto_coord(mouse_x), atto_coord(mouse_y), player] in cur_pieces):\n",
    "        x, y = atto_coord(mouse_x), atto_coord(mouse_y)\n",
    "\n",
    "        if click and current_time - last_click_time > 100:  # 100 milliseconds debounce\n",
    "            if selected_piece:\n",
    "                if x == sel_x and y == sel_y: #deselection\n",
    "                    selected_piece = False\n",
    "                    sel_x = -1\n",
    "                    sel_y = -1\n",
    "                    \n",
    "            else: #selection\n",
    "                selected_piece = True\n",
    "                sel_x = x\n",
    "                sel_y = y\n",
    "\n",
    "            last_click_time = current_time\n",
    "    \n",
    "    if click and current_time - last_click_time > 100 and [atto_coord(mouse_x), atto_coord(mouse_y)] in valid_moves and selected_piece:\n",
    "\n",
    "        cur_pieces.append([atto_coord(mouse_x), atto_coord(mouse_y),player])\n",
    "        player = -player\n",
    "        selected_piece = False\n",
    "    \n",
    "    # Draw hollow circles on valid moves if a piece is selected\n",
    "    if selected_piece:\n",
    "        for move in valid_moves:\n",
    "            px, py = atto_pixels(move[0]), atto_pixels(move[1])\n",
    "            s = pygame.Surface((SCREEN_SIZE, SCREEN_SIZE), pygame.SRCALPHA)\n",
    "            pygame.draw.circle(s, (113, 175, 255, 100) if player == 1 else (238, 167, 255, 100), (px, py), PIECE_SIZE, 3)  # Change color as needed\n",
    "            screen.blit(s, (0, 0))\n",
    "    return [sel_x, sel_y, player, selected_piece, last_click_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm commonly used in decision-making processes for games and other domains. It iteratively builds a search tree by simulating random plays and selecting moves based on statistical evaluation. The key steps of MCTS include selection, expansion, simulation, and backpropagation. This algorithm has proven effective in games like Go and chess, providing a balance between exploration and exploitation to find optimal strategies.\n",
    "\n",
    "For this project, and in the context of AlphaZero, this process is aided by a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    # Alpha Zero Node\n",
    "    ## Description:\n",
    "        A node for the AlphaZero MCTS. It contains the state, the action taken to get to the state, the prior probability of the action, the visit count, the value sum, and the children of the node.\n",
    "    ## Metohds:\n",
    "        - `is_expanded()`: Returns whether the node has been expanded.\n",
    "        - `select()`: Selects the best child node based on the UCB.\n",
    "        - `get_ucb()`: Returns the UCB of a child node.\n",
    "        - `expand()`: Expands the node by adding children.\n",
    "        - `backpropagate()`: Backpropagates the value of the node to the parent node.\n",
    "        '''\n",
    "    def __init__(self, game, args, state, player, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.player = player\n",
    "        self.prior = prior\n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_expanded(self):\n",
    "        '''\n",
    "        # is_expanded\n",
    "        ## Description:\n",
    "            Returns whether the node has been expanded.\n",
    "        ## Returns:\n",
    "            - `bool`: Whether the node has been expanded.'''\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        '''\n",
    "        # Description: \n",
    "        Selects the best child node from the current node's children in a Monte Carlo Tree Search using the Upper Confidence Bound (UCB) algorithm. \n",
    "\n",
    "        # Returns: \n",
    "        The best child node, chosen based on the highest UCB value or randomly if there's a tie.\n",
    "        '''\n",
    "        best_child = []\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = [child]\n",
    "                best_ucb = ucb\n",
    "            elif ucb == best_ucb:\n",
    "                best_child.append(child)\n",
    "                \n",
    "        return best_child[0] if len(best_child) == 1 else random.choice(best_child)\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        '''\n",
    "        # Description: \n",
    "        Calculates the Upper Confidence Bound (UCB) value for a given child node in a Monte Carlo Tree Search.\n",
    "\n",
    "        # Returns: \n",
    "        The calculated UCB value for the given child node.\n",
    "        '''\n",
    "        if child.visit_count == 0:\n",
    "            q_value = child.prior * self.args['C'] * (math.sqrt(self.visit_count)) / (child.visit_count + 1)\n",
    "        else:\n",
    "            q_value = -(child.value_sum / child.visit_count) + child.prior * self.args['C'] * (math.sqrt(self.visit_count)) / (child.visit_count + 1)\n",
    "        return q_value\n",
    "\n",
    "    def serialize(self):\n",
    "        # Serialize only essential data\n",
    "        node_data = {\n",
    "            'game': self.game,\n",
    "            'args': self.args,\n",
    "            'parent': self.parent,\n",
    "            'state': self.state,\n",
    "            'action_taken': self.action_taken,\n",
    "            'player': self.player,\n",
    "            'prior': self.prior,\n",
    "            'visit_count': self.visit_count,\n",
    "            'value_sum': self.value_sum,\n",
    "            'children': [child for child in self.children]  # Assuming each child has a unique ID\n",
    "        }\n",
    "        return json.dumps(node_data)\n",
    "\n",
    "\n",
    "    def deserialize(node_json):\n",
    "        # Convert JSON back into a Node object\n",
    "        node_data = json.loads(node_json)\n",
    "        node = Node(  # assuming constructor can handle this data\n",
    "            game = node_data['game'],\n",
    "            args = node_data['args'],\n",
    "            parent = node_data['parent'],\n",
    "            player = node_data['player'],\n",
    "            state=node_data['state'],\n",
    "            action_taken=node_data['action_taken'],\n",
    "            prior=node_data['prior'],\n",
    "            visit_count=node_data['visit_count'],\n",
    "        )\n",
    "        node.value_sum = node_data['value_sum']\n",
    "\n",
    "        for child in node_data['children']:\n",
    "            child.parent = node\n",
    "            node.children.append(child)\n",
    "\n",
    "        # You'll need to handle children reconstruction separately\n",
    "        return node\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        '''\n",
    "        # Description: \n",
    "        Expands the current node by adding new child nodes based on the given policy probabilities. For each possible action, it calculates the next state, adjusts the perspective for the opponent, and creates a new child node if the probability for that action is greater than zero.\n",
    "\n",
    "        # Returns: \n",
    "        None\n",
    "        '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "                child = Node(self.game, self.args, child_state, self.game.get_opponent(self.player), self, action, prob)\n",
    "                self.children.append(child)\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        '''\n",
    "        # Description: \n",
    "        Performs the backpropagation step in Monte Carlo Tree Search. It updates the current node's value sum and visit count based on the received value.\n",
    "\n",
    "        # Returns: \n",
    "        None\n",
    "        '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, game, args):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.tree_dict = {}\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, player):\n",
    "        \"\"\"\n",
    "        # Description:\n",
    "        Performs Monte Carlo Tree Search (MCTS) in batch to find the best action.\n",
    "\n",
    "        # Returns:\n",
    "        An array of arrays of action probabilities for each possible action.\n",
    "        \"\"\"\n",
    "\n",
    "        action_prob_list = []\n",
    "\n",
    "        for state in states:\n",
    "            \n",
    "            root = Node(self.game, self.args, state, player, visit_count=1)\n",
    "\n",
    "            searches = self.args['num_mcts_searches']\n",
    "\n",
    "            if str(state)+str(player) in self.tree_dict.keys():\n",
    "                action_prob_list.append(self.tree_dict.get(str(state)+str(player)))\n",
    "                continue\n",
    "            \n",
    "            policy, _ = self.model(\n",
    "                torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "            )\n",
    "            policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "            policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "                * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "                \n",
    "            valid_moves = self.game.get_valid_moves(state, player)\n",
    "\n",
    "            if self.args[\"game\"] == \"Attaxx\":\n",
    "                if np.sum(valid_moves) == 0:\n",
    "                    valid_moves[-1] = 1\n",
    "                else:\n",
    "                    valid_moves[-1] = 0\n",
    "\n",
    "            policy *= valid_moves\n",
    "            policy /= np.sum(policy)\n",
    "            root.expand(policy)\n",
    "                \n",
    "            for search in range(searches):\n",
    "                node = root\n",
    "                while node.is_expanded():\n",
    "                    node = node.select()\n",
    "                    \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken, node.player)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                    \n",
    "                if node.parent is not None:\n",
    "                    if node.action_taken == self.game.action_size - 1 and node.parent.action_taken == self.game.action_size - 1 and self.args['game'] == 'Go':\n",
    "                        is_terminal = True # if the action is pass when the previous action was also pass, end the game\n",
    "\n",
    "                if not is_terminal:\n",
    "                    policy, value = self.model(torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0))\n",
    "                    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                    valid_moves = self.game.get_valid_moves(node.state, player)\n",
    "\n",
    "                    if self.args[\"game\"] == \"Attaxx\":\n",
    "                        if np.sum(valid_moves) == 0:\n",
    "                            valid_moves[-1] = 1\n",
    "                        else:\n",
    "                            valid_moves[-1] = 0\n",
    "\n",
    "                    policy *= valid_moves\n",
    "                    policy /= np.sum(policy)\n",
    "                        \n",
    "                    value = value.item()\n",
    "                    node.expand(policy)\n",
    "\n",
    "                node.backpropagate(value)\n",
    "\n",
    "            action_probs = np.zeros(self.game.action_size)\n",
    "\n",
    "            for child in root.children:\n",
    "                action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "            action_probs /= np.sum(action_probs)\n",
    "            action_prob_list.append(action_probs)\n",
    "\n",
    "            self.tree_dict.update({str(state)+str(player): action_probs})\n",
    "\n",
    "        return action_prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "\n",
    "\n",
    "A Residual Neural Network (ResNet) is a deep learning architecture designed to address the challenges of training very deep networks. It introduces residual blocks with skip connections, enabling the network to skip layers during training. This helps in overcoming vanishing gradient issues, allowing the training of deep models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    # ResNet\n",
    "    ## Description:\n",
    "        A ResNet model for AlphaZero.\n",
    "        The model takes in a state and outputs a policy and value.\n",
    "         - The policy is a probability distribution over all possible actions.\n",
    "         - The value is a number between -1 and 1, where -1 means the current player loses and 1 means the current player wins following a tanh activation.\n",
    "        '''\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        # Description:\n",
    "        The forward pass of the model. This overrides the forward method of nn.Module so that it can be called directly on the model.\n",
    "\n",
    "        # Returns:\n",
    "        - `policy`: The policy output of the model.\n",
    "        - `value`: The value output of the model.\n",
    "        '''\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    '''\n",
    "    # Description:\n",
    "    A residual block for the ResNet model.\n",
    "    '''\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # Description:\n",
    "        Forward pass through the residual block.\n",
    "\n",
    "        # Returns:\n",
    "        Output tensor after passing through the block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaZero\n",
    "\n",
    "Alpha Zero is an algorithm introduced by DeepMind, starting from random play and given no domain knowledge except the game rules, a trained agent is capable of achieving superhuman level of performance, it completes this goal by combining a Monte Carlo Tree Search (MCTS) and a Neural Network in a policy iteration framework to achieve stable learning. Combining these elements an agent can then learn through self-play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(model, game, args)\n",
    "\n",
    "    def augment_state(self, state, probs):\n",
    "\n",
    "        augmented_states = []\n",
    "\n",
    "        skip_prob = probs[-1]\n",
    "        action_probs_matrix = np.array(probs[:-1]).reshape(self.game.column_count, self.game.row_count)\n",
    "        augmented_action_probs = []\n",
    "\n",
    "        def augment_and_append(transformed_state, transformed_probs_matrix):\n",
    "\n",
    "            # Append state\n",
    "            augmented_states.append(transformed_state)\n",
    "\n",
    "            # Flatten probs matrix, append the last value, and then append to augmented_action_probs\n",
    "            augmented_action_probs.append(list(transformed_probs_matrix.flatten()) + [skip_prob])\n",
    "\n",
    "        # Original state and probs\n",
    "        augment_and_append(state, action_probs_matrix)\n",
    "\n",
    "        # Rotate 90 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=1), np.rot90(action_probs_matrix, k=1))\n",
    "\n",
    "        # Rotate 180 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=2), np.rot90(action_probs_matrix, k=2))\n",
    "\n",
    "        # Rotate 270 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=3), np.rot90(action_probs_matrix, k=3))\n",
    "\n",
    "        # Flip horizontally\n",
    "        augment_and_append(np.fliplr(state), np.fliplr(action_probs_matrix))\n",
    "\n",
    "        # Flip vertically\n",
    "        augment_and_append(np.flipud(state), np.flipud(action_probs_matrix))\n",
    "\n",
    "        # Rotate 90 degrees clockwise and flip horizontally\n",
    "        augment_and_append(np.rot90(np.fliplr(state), k=1), np.rot90(np.fliplr(action_probs_matrix), k=1))\n",
    "\n",
    "        # Rotate 90 degrees clockwise and flip vertically\n",
    "        augment_and_append(np.rot90(np.flipud(state), k=1), np.rot90(np.flipud(action_probs_matrix), k=1))\n",
    "\n",
    "        return augmented_states, augmented_action_probs\n",
    "\n",
    "\n",
    "    def selfPlay(self):\n",
    "        player = 1\n",
    "\n",
    "        memory = []\n",
    "        states = []\n",
    "\n",
    "        for _ in range(0, self.args['parallel_games']):\n",
    "            state = self.game.get_initial_state()\n",
    "            states.append(state)\n",
    "            memory.append([])\n",
    "\n",
    "        iter = 0\n",
    "        prev_skip = False\n",
    "        temperature = self.args['temperature']\n",
    "        debugging = False\n",
    "\n",
    "        returnData = []\n",
    "\n",
    "        while True:\n",
    "            if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                print(\"\\nSEARCHING...\")\n",
    "\n",
    "            neutral_states_list = []\n",
    "\n",
    "            for state in states:\n",
    "                neutral_states_list.append(self.game.change_perspective(state, player))\n",
    "\n",
    "            action_probs_list = self.mcts.search(states, player)\n",
    "\n",
    "            for i, (neutral_state, action_probs) in enumerate(zip(neutral_states_list, action_probs_list)):\n",
    "                memory[i].append((neutral_state, action_probs, player))\n",
    "\n",
    "            for idx, (state, action_probs) in enumerate(zip(states, action_probs_list)):\n",
    "                temperature_action_probs = action_probs ** (1 / temperature)\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "                if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                    print(f\"Player: {player} with move {self.game.int_to_move(action)}\\nBoard:\")\n",
    "                    self.game.print_board(state)    \n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(state, action, player)\n",
    "                    \n",
    "\n",
    "                if action == self.game.action_size - 1 and self.args['game'] == 'Go':\n",
    "                    if prev_skip:\n",
    "                        is_terminal = True\n",
    "                    else:\n",
    "                        prev_skip = True\n",
    "                else:\n",
    "                    prev_skip = False\n",
    "\n",
    "                if is_terminal or iter >= self.args['max_moves']:\n",
    "                    returnMemory = []\n",
    "                    if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                        print(\"GAME OVER\\n\\n\")\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in memory[idx]:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "\n",
    "                        if self.args['augment']:\n",
    "                            augmented_states, augmented_action_probs = self.augment_state(hist_neutral_state, hist_action_probs)\n",
    "\n",
    "                            for augmented_state, augmented_probs in zip(augmented_states, augmented_action_probs):\n",
    "                                returnMemory.append((self.game.get_encoded_state(augmented_state), augmented_probs, hist_outcome))\n",
    "                        else:\n",
    "                            returnMemory.append((self.game.get_encoded_state(hist_neutral_state), hist_action_probs, hist_outcome))\n",
    "\n",
    "                        returnData = returnData + returnMemory\n",
    "\n",
    "                    del memory[idx]\n",
    "                    del states[idx]\n",
    "\n",
    "                if len(memory) <= 0:\n",
    "                    return returnData\n",
    "\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "            if temperature >= 0.1:\n",
    "                temperature = temperature * self.args['cooling_constant']\n",
    "            else:\n",
    "                temperature = 0.1\n",
    "\n",
    "            iter += 1\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self, memory = None, LAST_ITERATION=0):\n",
    "        primary_memory = []\n",
    "\n",
    "        if memory != None:\n",
    "            primary_memory = memory\n",
    "\n",
    "        for iteration in range(LAST_ITERATION+1, self.args['num_iterations']):\n",
    "            print(f\"Iteration {iteration + 1}\")\n",
    "\n",
    "            secondary_memory = []\n",
    "\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                states = self.selfPlay()\n",
    "                self.mcts.tree_dict = {}\n",
    "                secondary_memory += states\n",
    "\n",
    "            training_memory = []\n",
    "            if self.args['experience_replay']:\n",
    "                sample_size = int(len(primary_memory) * 0.3)\n",
    "\n",
    "                training_memory += random.sample(primary_memory, min(sample_size, len(primary_memory)))\n",
    "                training_memory += secondary_memory\n",
    "                \n",
    "                primary_memory += secondary_memory\n",
    "            else:\n",
    "                training_memory += secondary_memory\n",
    "\n",
    "            print(f\"Memory size: {len(training_memory)}\")\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(training_memory)\n",
    "\n",
    "            print(\"\\n\")\n",
    "                \n",
    "            torch.save(self.model.state_dict(), f\"DevelopmentModels/{self.args['alias']}/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"DevelopmentModels/{self.args['alias']}/optimizer_{iteration}.pt\")\n",
    "            with open(f'DevelopmentModels/{self.args[\"alias\"]}/memory_{iteration}.pkl', 'wb') as f:\n",
    "                pickle.dump(primary_memory, f)\n",
    "            print(\"Data Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "The following code was used both for training and testing the generated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "SAVE_NAME = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Go / Attaxx\n",
    "    GAME = \"Attaxx\"\n",
    "\n",
    "    # Board size (7/9 for Go, 4/5/6 for Attaxx)\n",
    "    SIZE = 6\n",
    "\n",
    "    # True to load previous model\n",
    "    # False to start from scratch\n",
    "    LOAD = True\n",
    "    LAST_ITERATION = -1\n",
    "\n",
    "    # Save Name\n",
    "    SAVE_NAME = \"a6x6\"\n",
    "\n",
    "    # False for training\n",
    "    # True for playing\n",
    "    TEST = True\n",
    "\n",
    "    # False if locally \n",
    "    # True if playing in the server\n",
    "    ONLINE = False\n",
    "\n",
    "    # Train from scratch\n",
    "    if not LOAD and not TEST:\n",
    "        LAST_ITERATION=-1\n",
    "\n",
    "    if GAME == 'Go':\n",
    "        if SIZE == 7:\n",
    "            args = {\n",
    "                'game': 'Go',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 16,                 # batch size for training\n",
    "                'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.90,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 2,                           # the value of the constant policy\n",
    "                'experience_replay': True,        # recycle a certain % of old random selfplay data in the current training iteration\n",
    "                'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                'parallel_games': 10,            # number of games run in parallel\n",
    "                'dirichlet_alpha': 0.03,          # the value of the dirichlet noise (alpha)\n",
    "                'dirichlet_epsilon': 0.25,        # the value of the dirichlet noise (epsilon)\n",
    "                'alias': ('Go' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Go(size = SIZE, komi = 5.5)\n",
    "            model = ResNet(game, 10, 10, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            \n",
    "        elif SIZE == 9:\n",
    "            args = {\n",
    "                'game': 'Go',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,    # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 60,                 # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 32,                 # batch size for training\n",
    "                'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.85,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 1,                           # the value of the constant policy\n",
    "                'experience_replay': True,        # recycle a certain % of old random selfplay data in the current training iteration\n",
    "                'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                'parallel_games': 5,            # number of games run in parallel\n",
    "                'dirichlet_alpha': 0.03,          # the value of the dirichlet noise (alpha)\n",
    "                'dirichlet_epsilon': 0.03,        # the value of the dirichlet noise (epsilon)\n",
    "                'alias': ('Go' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Go(size = SIZE, komi = 5.5)\n",
    "            model = ResNet(game, 9, 3, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    elif GAME == 'Attaxx':\n",
    "        game_size = [SIZE,SIZE]\n",
    "        if SIZE == 4:\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,  # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 100,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 10,                # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 16,                # batch size for training\n",
    "                'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.9,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 1,                           # the value of the constant policy\n",
    "                'dirichlet_alpha': 0.03,           # the value of the dirichlet noise\n",
    "                'dirichlet_epsilon': 0.03,       # the 001value of the dirichlet noise\n",
    "                'parallel_games': 10,            # number of games run in parallel\n",
    "                'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                'augment': False,                  # whether to augment the training data with flipped and rotated states\n",
    "                'alias': ('Attaxx' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 4, 8, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "        elif SIZE == 5:\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 10000,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,  # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 100,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 10,                # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 64,                # batch size for training\n",
    "                'temperature': 1,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.85,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 1,                           # the value of the constant policy\n",
    "                'dirichlet_alpha': 0.03,           # the value of the dirichlet noise\n",
    "                'dirichlet_epsilon': 0.03,       # the value of the dirichlet noise\n",
    "                'parallel_games': 15,            # number of games run in parallel\n",
    "                'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                'augment': False,                  # whether to augment the training data with flipped and rotated states\n",
    "                'alias': ('Attaxx' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 8, 16, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "        elif SIZE == 6:\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,  # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 100,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 20,                # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 128,                # batch size for training\n",
    "                'temperature': 1,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.85,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 1,                           # the value of the constant policy\n",
    "                'dirichlet_alpha': 0.03,           # the value of the dirichlet noise\n",
    "                'dirichlet_epsilon': 0.03,       # the value of the dirichlet noise\n",
    "                'parallel_games': 20,            # number of games run in parallel\n",
    "                'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                'augment': False,                  # whether to augment the training data with flipped and rotated states\n",
    "                'alias': ('Attaxx' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 12, 32, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    else:\n",
    "        print(\"Game Unavailable\")\n",
    "\n",
    "    if LOAD:\n",
    "        model.load_state_dict(torch.load(f'FinalModels/{SAVE_NAME}/{SAVE_NAME}.pt', map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(f'FinalModels/{SAVE_NAME}/o{SAVE_NAME}.pt', map_location=device))\n",
    "    \n",
    "        #with open(f'DevelopmentModels/{GAME+SAVE_NAME}/memory_{LAST_ITERATION}.pkl', 'rb') as f:\n",
    "         #   memory = pickle.load(f)\n",
    "    else:\n",
    "        memory = None\n",
    "\n",
    "    if not TEST:\n",
    "        os.makedirs(f'DevelopmentModels/{GAME+SAVE_NAME}', exist_ok=True)\n",
    "        alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "        alphaZero.learn(memory, LAST_ITERATION)\n",
    "\n",
    "    elif not ONLINE:\n",
    "\n",
    "        if not LOAD:\n",
    "            print(\"No model to test\")\n",
    "            exit()\n",
    "\n",
    "        global SIZE_BOARD, BLACK, WHITE, WOOD, SCREEN_SIZE\n",
    "\n",
    "        if GAME == 'Go':\n",
    "\n",
    "            \n",
    "            SIZE_BOARD = SIZE\n",
    "            BLACK = (0,0,0)\n",
    "            WHITE = (255,255,255)\n",
    "            WOOD = (216, 166, 91)\n",
    "            SCREEN_SIZE = 600\n",
    "            SCREEN_PADDING = 50\n",
    "            CELL_SIZE = (SCREEN_SIZE - SCREEN_PADDING) // SIZE_BOARD\n",
    "            PIECE_SIZE = (SCREEN_SIZE - 2*SCREEN_PADDING) // SIZE_BOARD // 3\n",
    "\n",
    "            PLAYER1 = \"AI\"\n",
    "            PLAYER2 = \"AI\"\n",
    "\n",
    "            mcts = MCTS(model, game, args)\n",
    "            state = game.get_initial_state()\n",
    "            #game.print_board(state)\n",
    "\n",
    "            player = 1\n",
    "            prev_skip = False\n",
    "            rendering = True\n",
    "            click = False\n",
    "            valid_moves = []\n",
    "\n",
    "            for i in range(SIZE_BOARD):\n",
    "                for j in range(SIZE_BOARD):\n",
    "                    valid_moves.append([i, j])\n",
    "\n",
    "            cur_pieces = []\n",
    "\n",
    "            a = None\n",
    "\n",
    "            if not rendering:\n",
    "                game.print_board(state)\n",
    "            else:\n",
    "                pygame.init()\n",
    "                #pygame_icon = pygame.image.load('image.png')\n",
    "                #pygame.display.set_icon(pygame_icon)\n",
    "\n",
    "                screen=pygame.display.set_mode((SCREEN_SIZE,SCREEN_SIZE))\n",
    "\n",
    "                pygame.display.set_caption(\"Go\")\n",
    "\n",
    "            while True:\n",
    "\n",
    "                if rendering:\n",
    "                    for event in pygame.event.get():\n",
    "                        if event.type == pygame.QUIT:\n",
    "                            pygame.quit()\n",
    "                        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                            click = True\n",
    "                            a, b, player = gohover_to_select(player, valid_moves, click)\n",
    "                        if event.type == pygame.MOUSEBUTTONUP:\n",
    "                            click = False\n",
    "\n",
    "                    screen.fill(WOOD)\n",
    "                    godraw_board()\n",
    "                    for i in range(0,len(state)):\n",
    "                        for j in range(0,len(state)):\n",
    "                            if state[i][j] == 0:\n",
    "                                if game.is_valid_move(state, (i,j), player):\n",
    "                                    valid_moves.append([i,j])\n",
    "                            else:\n",
    "                                godraw_piece(i,j, state[i][j])\n",
    "\n",
    "                if player == 1:\n",
    "                    \n",
    "                    if PLAYER1 == 'user':\n",
    "\n",
    "                        valid_move_selected = False\n",
    "\n",
    "                        a, b, player = gohover_to_select(player, valid_moves, click)\n",
    "\n",
    "                        if click:\n",
    "\n",
    "                            mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                            a, b = goto_coord(mouse_x), goto_coord(mouse_y)\n",
    "\n",
    "                            action = a * SIZE + b\n",
    "\n",
    "                            state = game.get_next_state(state, action, player)\n",
    "\n",
    "                            winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                \n",
    "                            if action == game.action_size:\n",
    "                                if prev_skip:\n",
    "                                    win = True\n",
    "                                else:\n",
    "                                    prev_skip = True\n",
    "                            else:\n",
    "                                prev_skip = False\n",
    "\n",
    "                            if win:\n",
    "                                print(f\"player {winner} wins\")\n",
    "                                break\n",
    "\n",
    "                            player = -player\n",
    "\n",
    "                    else:\n",
    "                        tmp_state = game.change_perspective(state, -1)\n",
    "                        action = mcts.search([tmp_state], -player)                    \n",
    "                        action = np.argmax(action[0])\n",
    "                        print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "\n",
    "                        winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                \n",
    "                        if action == game.action_size:\n",
    "                            if prev_skip:\n",
    "                                win = True\n",
    "                            else:\n",
    "                                prev_skip = True\n",
    "                        else:\n",
    "                            prev_skip = False\n",
    "\n",
    "                        if win:\n",
    "                            print(f\"player {winner} wins\")\n",
    "                            break\n",
    "\n",
    "                        player = -player\n",
    "                else:\n",
    "                    if PLAYER2 == 'user':\n",
    "                        valid_move_selected = False\n",
    "\n",
    "                        a, b, player = gohover_to_select(player, valid_moves, click)\n",
    "\n",
    "                        if click:\n",
    "\n",
    "                            mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                            a, b = goto_coord(mouse_x), goto_coord(mouse_y)\n",
    "\n",
    "                            action = a * SIZE + b\n",
    "\n",
    "                            state = game.get_next_state(state, action, player)\n",
    "\n",
    "                            winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                \n",
    "                            if action == game.action_size:\n",
    "                                if prev_skip:\n",
    "                                    win = True\n",
    "                                else:\n",
    "                                    prev_skip = True\n",
    "                            else:\n",
    "                                prev_skip = False\n",
    "\n",
    "                            if win:\n",
    "                                print(f\"player {winner} wins\")\n",
    "                                break\n",
    "\n",
    "                            player = -player\n",
    "                    else:\n",
    "                        action = mcts.search([state], player)                    \n",
    "                        action = np.argmax(action[0])\n",
    "                        \n",
    "\n",
    "                        print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "\n",
    "                        winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                \n",
    "                        if action == game.action_size:\n",
    "                            if prev_skip:\n",
    "                                win = True\n",
    "                            else:\n",
    "                                prev_skip = True\n",
    "                        else:\n",
    "                            prev_skip = False\n",
    "\n",
    "                        if win:\n",
    "                            print(f\"player {winner} wins\")\n",
    "                            break\n",
    "\n",
    "                        player = -player\n",
    "\n",
    "                pygame.display.flip()\n",
    "\n",
    "        elif GAME == 'Attaxx':\n",
    "            PLAYER1 = \"AI\"\n",
    "            PLAYER2 = \"AI\"\n",
    "            SIZE_BOARD = SIZE\n",
    "\n",
    "            RED = (238, 167, 255)\n",
    "            BLUE = (113, 175, 255)\n",
    "            GRAY = (115, 115, 115)\n",
    "            BLACK = (0, 0, 0)\n",
    "\n",
    "            pygame.init()\n",
    "\n",
    "            SCREEN_SIZE=600\n",
    "            SCREEN_PADDING = 100\n",
    "            CELL_SIZE = (SCREEN_SIZE - SCREEN_PADDING) // SIZE_BOARD\n",
    "            PIECE_SIZE = (SCREEN_SIZE - 2*SCREEN_PADDING) // SIZE_BOARD // 3\n",
    "\n",
    "            screen=pygame.display.set_mode((SCREEN_SIZE,SCREEN_SIZE))\n",
    "\n",
    "            pygame.display.set_caption(\"Attaxx\")\n",
    "            click = False\n",
    "            valid_moves = []\n",
    "            for i in range(SIZE_BOARD):\n",
    "                for j in range(SIZE_BOARD):\n",
    "                    valid_moves.append([i, j])\n",
    "\n",
    "            cur_pieces =[]\n",
    "\n",
    "            player = 1\n",
    "            selected_piece = False\n",
    "            last_click_time = 0\n",
    "            x, y = -1, -1\n",
    "\n",
    "            mcts = MCTS(model, game, args)\n",
    "            state = game.get_initial_state()\n",
    "            sel = False\n",
    "            while True:\n",
    "\n",
    "                screen.fill(GRAY)\n",
    "                atdraw_board()\n",
    "\n",
    "                for i in range(0,len(state)):\n",
    "                        for j in range(0,len(state)):\n",
    "                            if state[i][j] != 0:\n",
    "                                atdraw_piece(i,j, state[i][j])\n",
    "\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        click = True\n",
    "                        x, y, player, selected_piece, last_click_time = athover_to_select(x, y, player, valid_moves, click, selected_piece, cur_pieces, last_click_time)\n",
    "                        pygame.time.delay(50)\n",
    "                    if event.type == pygame.MOUSEBUTTONUP:\n",
    "                        click = False\n",
    "                        pygame.time.delay(50)\n",
    "\n",
    "\n",
    "                for piece in cur_pieces:\n",
    "                    atdraw_piece(piece[0], piece[1], piece[2])\n",
    "\n",
    "                if player == 1:\n",
    "\n",
    "                    if PLAYER1 == 'user':\n",
    "                        x, y, player, selected_piece, last_click_time = athover_to_select(x, y, player, valid_moves, click, selected_piece, cur_pieces, last_click_time)\n",
    "                        if x != -1 and y != -1:\n",
    "                            valid_moves = []\n",
    "                            for i in range(0,len(state)):\n",
    "                                for j in range(0,len(state)):\n",
    "                                    if game.is_valid_move(state, (x,y,i,j), player):\n",
    "                                        valid_moves.append([i,j])\n",
    "\n",
    "                        if sel:\n",
    "                            for move in valid_moves:\n",
    "                                px, py = atto_pixels(move[0]), atto_pixels(move[1])\n",
    "                                s = pygame.Surface((SCREEN_SIZE, SCREEN_SIZE), pygame.SRCALPHA)\n",
    "                                pygame.draw.circle(s, (113, 175, 255, 100) if player == 1 else (238, 167, 255, 100), (px, py), PIECE_SIZE, 3)  # Change color as needed\n",
    "                                screen.blit(s, (0, 0))\n",
    "\n",
    "                        if click and sel:\n",
    "                            mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                            a, b = atto_coord(mouse_x), atto_coord(mouse_y)\n",
    "                            if a == x and b == y:\n",
    "                                sel = False\n",
    "                            else:\n",
    "                                move = (x,y,atto_coord(mouse_x), atto_coord(mouse_y))\n",
    "\n",
    "                                action = game.move_to_int(move)\n",
    "                                state = game.get_next_state(state, action, player)\n",
    "\n",
    "                                winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                                if win:\n",
    "                                    game.print_board(state)\n",
    "                                    print(f\"player {winner} wins\")\n",
    "                                    pygame.quit()\n",
    "                                    break\n",
    "                                \n",
    "                                player = -player\n",
    "                                x = -1\n",
    "                                y = -1\n",
    "                                sel = False\n",
    "                        \n",
    "                        if click and not sel:\n",
    "                            mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                            x, y = atto_coord(mouse_x), atto_coord(mouse_y)\n",
    "                            if state[x,y] == player:\n",
    "                                sel = True\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        tmp_state = game.change_perspective(state, -1)\n",
    "                        action = mcts.search([tmp_state], -player)\n",
    "                        action = np.argmax(action)\n",
    "                        print(f\"\\nAlphaZero Action: {game.int_to_move(action)}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                        winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                        if win:\n",
    "                            game.print_board(state)\n",
    "                            print(f\"player {winner} wins\")\n",
    "                            pygame.quit()\n",
    "                            break\n",
    "\n",
    "                        player = -player\n",
    "\n",
    "                else:\n",
    "                    if PLAYER2 == 'user':\n",
    "                        x, y, player, selected_piece, last_click_time = athover_to_select(x, y, player, valid_moves, click, selected_piece, cur_pieces, last_click_time)\n",
    "                        if x != -1 and y != -1:\n",
    "                            valid_moves = []\n",
    "                            for i in range(0,len(state)):\n",
    "                                for j in range(0,len(state)):\n",
    "                                    if game.is_valid_move(state, (x,y,i,j), player):\n",
    "                                        valid_moves.append([i,j])\n",
    "\n",
    "                        if sel:\n",
    "                            for move in valid_moves:\n",
    "                                px, py = atto_pixels(move[0]), atto_pixels(move[1])\n",
    "                                s = pygame.Surface((SCREEN_SIZE, SCREEN_SIZE), pygame.SRCALPHA)\n",
    "                                pygame.draw.circle(s, (113, 175, 255, 100) if player == 1 else (238, 167, 255, 100), (px, py), PIECE_SIZE, 3)  # Change color as needed\n",
    "                                screen.blit(s, (0, 0))\n",
    "\n",
    "                        if click and sel:\n",
    "                            mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                            a, b = atto_coord(mouse_x), atto_coord(mouse_y)\n",
    "                            if a == x and b == y:\n",
    "                                sel = False\n",
    "                            else:\n",
    "                                move = (x,y,atto_coord(mouse_x), atto_coord(mouse_y))\n",
    "\n",
    "                                action = game.move_to_int(move)\n",
    "                                state = game.get_next_state(state, action, player)\n",
    "\n",
    "                                winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                                if win:\n",
    "                                    game.print_board(state)\n",
    "                                    print(f\"player {winner} wins\")\n",
    "                                    pygame.quit()\n",
    "                                    break\n",
    "                                print(\"brug\")\n",
    "                                player = -player\n",
    "                                x = -1\n",
    "                                y = -1\n",
    "                                sel = False\n",
    "                        \n",
    "                        if click and not sel:\n",
    "                            mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                            x, y = atto_coord(mouse_x), atto_coord(mouse_y)\n",
    "                            if state[x,y] == player:\n",
    "                                sel = True\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        tmp_state = game.change_perspective(state, -1)\n",
    "                        action = mcts.search([tmp_state], -player)\n",
    "                        action = np.argmax(action)\n",
    "                        print(f\"\\nAlphaZero Action: {game.int_to_move(action)}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                        winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                        if win:\n",
    "                            game.print_board(state)\n",
    "                            print(f\"player {winner} wins\")\n",
    "                            pygame.quit()\n",
    "                            break\n",
    "\n",
    "                        player = -player\n",
    "\n",
    "\n",
    "\n",
    "                pygame.display.flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Client Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLINE = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if ONLINE:\n",
    "    \n",
    "    mode=\"A6x6\" # \"A6x6\" \"G7x7\" \"G9x9\" \"A5x5\"\n",
    "\n",
    "    def load_model():\n",
    "\n",
    "        global model, optimizer, mcts, game, args, state\n",
    "        game_size = [mode[1],mode[1]]\n",
    "        if mode[0] == \"A\":\n",
    "            game_size = [int(mode[1]),int(mode[1])]\n",
    "\n",
    "            if int(mode[1]) == 4:\n",
    "                args = {\n",
    "                    'game': 'Attaxx',\n",
    "                    'num_iterations': 1,              # number of highest level iterations\n",
    "                    'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                    'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                    'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                    'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                    'batch_size': 16,                 # batch size for training\n",
    "                    'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                    'cooling_constant': 0.9,          # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                    'C': 2,                           # the value of the constant policy\n",
    "                    'dirichlet_alpha': 0.03,          # the value of the dirichlet noise\n",
    "                    'dirichlet_epsilon': 0.25,        # the 001value of the dirichlet noise\n",
    "                    'parallel_games': 10,             # number of games run in parallel\n",
    "                    'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                    'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                }\n",
    "\n",
    "                game = Attaxx(game_size)\n",
    "                model = ResNet(game, 4, 8, device)\n",
    "                optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "            elif int(mode[1]) == 5:\n",
    "                args = {\n",
    "                    'game': 'Attaxx',\n",
    "                    'num_iterations': 1,              # number of highest level iterations\n",
    "                    'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                    'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                    'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                    'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                    'batch_size': 16,                 # batch size for training\n",
    "                    'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                    'cooling_constant': 0.9,          # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                    'C': 2,                           # the value of the constant policy\n",
    "                    'dirichlet_alpha': 0.03,          # the value of the dirichlet noise\n",
    "                    'dirichlet_epsilon': 0.25,        # the 001value of the dirichlet noise\n",
    "                    'parallel_games': 10,             # number of games run in parallel\n",
    "                    'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                    'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                }\n",
    "\n",
    "                game = Attaxx(game_size)\n",
    "                model = ResNet(game, 8, 16, device)\n",
    "                optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "            elif int(mode[1]) == 6:\n",
    "                args = {\n",
    "                    'game': 'Attaxx',\n",
    "                    'num_iterations': 1,              # number of highest level iterations\n",
    "                    'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                    'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                    'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                    'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                    'batch_size': 16,                 # batch size for training\n",
    "                    'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                    'cooling_constant': 0.9,          # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                    'C': 2,                           # the value of the constant policy\n",
    "                    'dirichlet_alpha': 0.03,          # the value of the dirichlet noise\n",
    "                    'dirichlet_epsilon': 0.25,        # the 001value of the dirichlet noise\n",
    "                    'parallel_games': 10,             # number of games run in parallel\n",
    "                    'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                    'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                }\n",
    "\n",
    "                game = Attaxx(game_size)\n",
    "                model = ResNet(game, 12, 32, device)\n",
    "                optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "                \n",
    "            state = game.get_initial_state()\n",
    "\n",
    "            if int(mode[1]) == 4:\n",
    "                model.load_state_dict(torch.load(f'FinalModels/a4x4/a4x4.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'FinalModels/a4x4/oa4x4.pt', map_location=device))\n",
    "                print(\"Successfuly loaded Attaxx 4x4\")\n",
    "            elif int(mode[1]) == 5:\n",
    "                model.load_state_dict(torch.load(f'FinalModels/a5x5/a5x5.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'FinalModels/a5x5/oa5x5.pt', map_location=device))\n",
    "                print(\"Successfuly loaded Attaxx 5x5\")\n",
    "            elif int(mode[1]) == 6:\n",
    "                model.load_state_dict(torch.load(f'FinalModels/a6x6/a6x6.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'FinalModels/a6x6/oa6x6.pt', map_location=device))\n",
    "                print(\"Successfuly loaded Attaxx 6x6\")\n",
    "\n",
    "            mcts = MCTS(model, game, args)\n",
    "            \n",
    "        elif mode[0] == \"G\":\n",
    "            game_size = int(mode[1])\n",
    "\n",
    "            if game_size == 7:\n",
    "                args = {\n",
    "                    'game': 'Go',\n",
    "                    'num_iterations': 1,              # number of highest level iterations\n",
    "                    'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                    'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                    'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                    'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                    'batch_size': 16,                 # batch size for training\n",
    "                    'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                    'cooling_constant': 0.9,          # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                    'C': 2,                           # the value of the constant policy\n",
    "                    'dirichlet_alpha': 0.03,          # the value of the dirichlet noise\n",
    "                    'dirichlet_epsilon': 0.25,        # the 001value of the dirichlet noise\n",
    "                    'parallel_games': 10,             # number of games run in parallel\n",
    "                    'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                    'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                }\n",
    "                print('here')\n",
    "                game = Go(size = int(mode[1]), komi = 5.5)\n",
    "                model = ResNet(game, 10, 10, device)\n",
    "                optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            \n",
    "            elif game_size == 9:\n",
    "                args = {\n",
    "                    'game': 'Go',\n",
    "                    'num_iterations': 1,              # number of highest level iterations\n",
    "                    'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                    'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                    'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                    'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                    'batch_size': 16,                 # batch size for training\n",
    "                    'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                    'cooling_constant': 0.9,          # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                    'C': 2,                           # the value of the constant policy\n",
    "                    'dirichlet_alpha': 0.03,          # the value of the dirichlet noise\n",
    "                    'dirichlet_epsilon': 0.25,        # the 001value of the dirichlet noise\n",
    "                    'parallel_games': 10,             # number of games run in parallel\n",
    "                    'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                    'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                }\n",
    "\n",
    "                game = Go(size = int(mode[1]), komi = 5.5)\n",
    "                model = ResNet(game, 9, 3, device)\n",
    "                optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "            state = game.get_initial_state()\n",
    "\n",
    "            if game_size == 7:\n",
    "                model.load_state_dict(torch.load(f'FinalModels/g7x7/g7x7.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'FinalModels/g7x7/og7x7.pt', map_location=device))\n",
    "                print(\"Successfuly loaded Go 7x7\")\n",
    "            elif game_size == 9:\n",
    "                model.load_state_dict(torch.load(f'FinalModels/g9x9/g9x9.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'FinalModels/g9x9/og9x9.pt', map_location=device))\n",
    "                print(\"Successfuly loaded Go 9x9\")\n",
    "\n",
    "            mcts = MCTS(model, game, args)\n",
    "\n",
    "    def generate_move():\n",
    "        \n",
    "        if mode[0] == \"A\":\n",
    "            if player == 1:\n",
    "                tmp_state = game.change_perspective(state, -1)\n",
    "                action = mcts.search([tmp_state], -player)\n",
    "                action = np.argmax(action)\n",
    "            else:\n",
    "                action = mcts.search(state, player)\n",
    "                action = np.argmax(action)\n",
    "\n",
    "            move = game.int_to_move(action)\n",
    "            print(f\"\\nAlphaZero Action: {move}\\n\")\n",
    "            \n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "            return f\"MOVE {move[0]} {move[1]} {move[2]} {move[3]}\"\n",
    "        \n",
    "        else:\n",
    "            if player == 1:\n",
    "                tmp_state = game.change_perspective(state, -1)\n",
    "                action = mcts.search([tmp_state], -player)\n",
    "                action = np.argmax(action)\n",
    "            else:\n",
    "                action = mcts.search(state, player)\n",
    "                action = np.argmax(action)\n",
    "\n",
    "            print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "            return f\"MOVE {action // game.row_count} {action % game.column_count}\"\n",
    "\n",
    "    def apply_opponent_move(response):\n",
    "\n",
    "        numbers = [int(x) for x in response.split()[1:]]\n",
    "\n",
    "        if mode[-1] == \"A\":\n",
    "            action = game.move_to_int((response[0],response[1],response[2],response[3]))\n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "        else:\n",
    "            action = numbers[0] * game.row_count + numbers[1]\n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "    def connect_to_server(host='localhost', port=12345):\n",
    "        global player\n",
    "        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        client_socket.connect((host, port))\n",
    "        \n",
    "        response = client_socket.recv(1024).decode()\n",
    "        print(f\"Server ResponseINIT: {response}\")\n",
    "        \n",
    "        mode = response[-4:]\n",
    "        print(\"Playing:\", mode)\n",
    "\n",
    "        load_model()\n",
    "        \n",
    "        if \"1\" in response:\n",
    "            player=1\n",
    "        else:\n",
    "            player=-1\n",
    "            first=True\n",
    "        \n",
    "        while True:\n",
    "            # Generate and send a random move\n",
    "            if player == 1 or not first:\n",
    "                move = generate_move()\n",
    "                player = -player\n",
    "                time.sleep(1)\n",
    "                client_socket.send(move.encode())\n",
    "                print(\"Send:\",move)\n",
    "            \n",
    "                # Wait for server response\n",
    "                response = client_socket.recv(1024).decode()\n",
    "                apply_opponent_move(response)\n",
    "                print(f\"Server Response1: {response}\")\n",
    "                if \"END\" in response: break\n",
    "            \n",
    "            first = False\n",
    "            response = client_socket.recv(1024).decode()\n",
    "            apply_opponent_move(response)\n",
    "            print(f\"Server Response2: {response}\")\n",
    "            if \"END\" in response: break\n",
    "\n",
    "            player = -player\n",
    "\n",
    "        client_socket.close()\n",
    "\n",
    "    connect_to_server()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isia2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
