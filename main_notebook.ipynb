{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Zero For Generalized Game Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pygame\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go is an ancient board game that is believed to be originated in China over 4000 years ago. It's usually played on a 19x19 grid board by two players, one using black stones and the other white stones. The game is known for its simple rules but depth in strategy. The primary objective in Go is to control more territory on the board than your opponent and this is gained by surrounding empty areas of the board with your stones. Players also aim to capture their opponent's stones by completely surrounding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Go():\n",
    "\n",
    "    EMPTY = 0\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "    BLACKMARKER = 4\n",
    "    WHITEMARKER = 5\n",
    "    LIBERTY = 8\n",
    "\n",
    "    def __init__(self, size, komi):\n",
    "        self.row_count = size\n",
    "        self.column_count = size\n",
    "        self.komi = 5.5\n",
    "        self.action_size = self.row_count * self.column_count + 1\n",
    "        self.liberties = []\n",
    "        self.block = []\n",
    "        self.seki_liberties = []\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns a board of the argument size filled of zeros.\n",
    "\n",
    "        # Retuns:\n",
    "        Empty board full of zeros\n",
    "        '''\n",
    "        board = np.zeros((self.row_count, self.column_count))\n",
    "        return board\n",
    "    \n",
    "\n",
    "    def count(self, x, y, state: list, player:int , liberties: list, block: list) -> tuple[list, list]:\n",
    "        '''\n",
    "        # Description:\n",
    "        Counts the number of liberties of a stone and the number of stones in a block.\n",
    "        Follows a recursive approach to count the liberties of a stone and the number of stones in a block.\n",
    "\n",
    "        # Returns:\n",
    "        A tuple containing the number of liberties and the number of stones in a block.\n",
    "        '''\n",
    "        \n",
    "        #initialize piece\n",
    "        piece = state[y][x]\n",
    "        #if there's a stone at square of the given player\n",
    "        if piece == player:\n",
    "            #save stone coords\n",
    "            block.append((y,x))\n",
    "            #mark the stone\n",
    "            if player == self.BLACK:\n",
    "                state[y][x] = self.BLACKMARKER\n",
    "            else:\n",
    "                state[y][x] = self.WHITEMARKER\n",
    "            \n",
    "            #look for neighbours recursively\n",
    "            if y-1 >= 0:\n",
    "                liberties, block = self.count(x,y-1,state,player,liberties, block) #walk north\n",
    "            if x+1 < self.column_count:\n",
    "                liberties, block = self.count(x+1,y,state,player,liberties, block) #walk east\n",
    "            if y+1 < self.row_count:\n",
    "                liberties, block = self.count(x,y+1,state,player,liberties, block) #walk south\n",
    "            if x-1 >= 0:\n",
    "                liberties, block = self.count(x-1,y,state,player,liberties, block) #walk west\n",
    "\n",
    "        #if square is empty\n",
    "        elif piece == self.EMPTY:\n",
    "            #mark liberty\n",
    "            state[y][x] = self.LIBERTY\n",
    "            #save liberties\n",
    "            liberties.append((y,x))\n",
    "\n",
    "        # print(\"Liberties: \" + str(len(self.liberties)) + \" in: \" + str(x) + \",\" + str(y))\n",
    "        # print(\"Block: \" + str(len(self.block)) + \" in: \" + str(x) + \",\" + str(y))\n",
    "        return liberties, block\n",
    "\n",
    "    #remove captured stones\n",
    "    def clear_block(self, block: list, state: list) -> list:\n",
    "        '''\n",
    "        # Description:\n",
    "        Clears the block of stones captured by the opponent on the board.\n",
    "\n",
    "        # Returns:\n",
    "        The board with the captured stones removed.\n",
    "        '''\n",
    "\n",
    "        #clears the elements in the block of elements which is captured\n",
    "        for i in range(len(block)): \n",
    "            y, x = block[i]\n",
    "            state[y][x] = self.EMPTY\n",
    "        \n",
    "        return state\n",
    "\n",
    "    #restore board after counting stones and liberties\n",
    "    def restore_board(self, state: list) -> list:\n",
    "        '''\n",
    "        # Description:\n",
    "        Restores the board to its original state after counting liberties and stones.\n",
    "        This is done by unmarking the stones following bitwise operations with the global class variables.\n",
    "        \n",
    "        # Returns:\n",
    "        The board with the stones unmarked.\n",
    "        '''\n",
    "\n",
    "        #unmark stones\n",
    "        # print(\"Restore Board\")\n",
    "        # print(state)\n",
    "        for y in range(len(state)):\n",
    "            for x in range(len(state)):\n",
    "                #restore piece\n",
    "                val = state[y][x]\n",
    "                if val == self.BLACKMARKER:\n",
    "                    state[y][x] = self.BLACK\n",
    "                elif val == self.WHITEMARKER:\n",
    "                    state[y][x] = self.WHITE\n",
    "                elif val == self.LIBERTY:\n",
    "                    state[y][x] = self.EMPTY\n",
    "\n",
    "        # print(\"After Restore Board\")\n",
    "        # print(state)\n",
    "        return state\n",
    "\n",
    "    def print_board(self, state: list) -> None:\n",
    "            '''\n",
    "            # Description:\n",
    "            Draws the board in the console.\n",
    "\n",
    "            # Returns:\n",
    "            None\n",
    "            '''\n",
    "\n",
    "        # Print column coordinates\n",
    "            print(\"   \", end=\"\")\n",
    "            for j in range(len(state[0])):\n",
    "                print(f\"{j:2}\", end=\" \")\n",
    "            print(\"\\n  +\", end=\"\")\n",
    "            for _ in range(len(state[0])):\n",
    "                print(\"---\", end=\"\")\n",
    "            print()\n",
    "\n",
    "            # Print rows with row coordinates\n",
    "            for i in range(len(state)):\n",
    "                print(f\"{i:2}|\", end=\" \")\n",
    "                for j in range(len(state[0])):\n",
    "                    print(f\"{str(int(state[i][j])):2}\", end=\" \")\n",
    "                print()\n",
    "    \n",
    "    def captures(self, state: list,player: int, a:int, b:int) -> tuple[bool, list]:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks if a move causes a capture of stones of the player passed as an argument.\n",
    "        If a move causes a capture, the stones are removed from the board.\n",
    "\n",
    "        # Returns:\n",
    "        A tuple containing a boolean indicating if a capture has been made and the board with the captured stones removed.\n",
    "        '''\n",
    "        check = False\n",
    "        neighbours = []\n",
    "        if(a > 0): neighbours.append((a-1, b))\n",
    "        if(a < self.column_count - 1): neighbours.append((a+1, b))\n",
    "        if(b > 0): neighbours.append((a, b - 1))\n",
    "        if(b < self.row_count - 1): neighbours.append((a, b+1))\n",
    "\n",
    "        #loop over the board squares\n",
    "        for pos in neighbours:\n",
    "            # print(pos)\n",
    "            x = pos[0]\n",
    "            y = pos[1]    \n",
    "            # init piece\n",
    "            piece = state[x][y]\n",
    "\n",
    "                #if stone belongs to given colour\n",
    "            if piece == player:\n",
    "                # print(\"opponent piece\")\n",
    "                # count liberties\n",
    "                liberties = []\n",
    "                block = []\n",
    "                liberties, block = self.count(y, x, state, player, liberties, block)\n",
    "                # print(\"Liberties in count: \" + str(len(liberties)))\n",
    "                # if no liberties remove the stones\n",
    "                if len(liberties) == 0: \n",
    "                    #clear block\n",
    "                    state = self.clear_block(block, state)\n",
    "                    check = True\n",
    "\n",
    "                #restore the board\n",
    "                state = self.restore_board(state)\n",
    "\n",
    "        #print(\"Captures: \" + str(check))\n",
    "        return check, state\n",
    "    \n",
    "    def set_stone(self, a, b, state, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Places the piece on the board. THIS DOES NOT account for the rules of the game, use get_next_state().\n",
    "\n",
    "        # Retuns:\n",
    "        Board with the piece placed.\n",
    "        '''\n",
    "        state[a][b] = player\n",
    "        return state\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        '''\n",
    "        # Description\n",
    "        Plays the move, verifies and undergoes captures and saves the state to the history.\n",
    "        \n",
    "        # Returns:\n",
    "        New state with everything updated.\n",
    "        '''\n",
    "        if action == self.row_count * self.column_count:\n",
    "            return state # pass move\n",
    "\n",
    "        a = action // self.row_count\n",
    "        b = action % self.column_count\n",
    "\n",
    "        # checking if the move is part of is the secondary move to a ko fight\n",
    "        state = self.set_stone(a, b, state, player)\n",
    "        # print(state)\n",
    "        state = self.captures(state, -player, a, b)[1]\n",
    "        return state\n",
    "    \n",
    "    def is_valid_move(self, state: list, action: tuple, player: int) -> bool:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks if a move is valid.\n",
    "        If a move repeats a previous state or commits suicide (gets captured without capturing back), it is not valid.\n",
    "        \n",
    "        A print will follow explaining the invalid move in case it exists.\n",
    "\n",
    "        # Returns:\n",
    "        A boolean confirming the validity of the move.\n",
    "        '''\n",
    "\n",
    "        a = action[0]\n",
    "        b = action[1]\n",
    "\n",
    "        #print(f\"{a} , {b}\")\n",
    "\n",
    "        statecopy = np.copy(state).astype(np.int8)\n",
    "\n",
    "        if state[a][b] != self.EMPTY:\n",
    "            # print(\"Space Occupied\")\n",
    "            return False \n",
    "\n",
    "\n",
    "        statecopy = self.set_stone(a,b,statecopy,player)\n",
    "\n",
    "        if self.captures(statecopy, -player, a, b)[0] == True:\n",
    "            return True\n",
    "        else:\n",
    "            #print(\"no captures\")\n",
    "            libs, block = self.count(b,a,statecopy,player,[],[])\n",
    "            #print(libs)\n",
    "            if len(libs) == 0:\n",
    "                #print(\"Invalid, Suicide\")\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        \n",
    "\n",
    "    def get_valid_moves(self, state, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns a matrix with the valid moves for the current player.\n",
    "        '''\n",
    "        newstate = np.zeros((self.row_count, self.column_count))\n",
    "        for a in range(0, self.column_count):\n",
    "            for b in range(0, self.row_count):\n",
    "                if self.is_valid_move(state, (a,b), player):\n",
    "                    newstate[a][b] = 1\n",
    "        \n",
    "        newstate = newstate.reshape(-1)\n",
    "\n",
    "        empty = 0\n",
    "        endgame = True\n",
    "        \n",
    "        for x in range(self.column_count):\n",
    "            for y in range(self.row_count):\n",
    "                if state[x][y] == self.EMPTY:\n",
    "                    empty += 1\n",
    "                    if empty >= self.column_count * self.row_count // 4: # if more than 1/4 of the board is empty, it is not the endgame\n",
    "                        endgame = False\n",
    "                        break\n",
    "        if endgame:\n",
    "            newstate = np.concatenate([newstate, [1]])\n",
    "        else:\n",
    "            newstate = np.concatenate([newstate, [0]])\n",
    "        return (newstate).astype(np.int8)\n",
    "\n",
    "    def get_value_and_terminated(self, state, action, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns the value of the state and if the game is over.\n",
    "        '''\n",
    "\n",
    "        scoring, endgame = self.scoring(state)\n",
    "\n",
    "        if endgame:\n",
    "            if player == self.BLACK:\n",
    "                if scoring > 0:\n",
    "                    return 1, True\n",
    "                else:\n",
    "                    return -1, True\n",
    "            else:\n",
    "                if scoring < 0:\n",
    "                    return 1, True\n",
    "                else:\n",
    "                    return -1, True\n",
    "        else:\n",
    "            if player == self.BLACK:\n",
    "                if scoring > 0:\n",
    "                    return 1, False\n",
    "                else:\n",
    "                    return -1, False\n",
    "            else:\n",
    "                if scoring < 0:\n",
    "                    return 1, False\n",
    "                else:\n",
    "                    return -1, False\n",
    "\n",
    "\n",
    "        \n",
    "    def scoring(self, state: list) -> int:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks the score of the game. Score is calculated using:\n",
    "\n",
    "        black - (white + komi)\n",
    "\n",
    "        # Returns:\n",
    "        Integer with score.\n",
    "        '''\n",
    "        black = 0\n",
    "        white = 0\n",
    "        empty = 0\n",
    "        endgame = True\n",
    "\n",
    "        for x in range(self.column_count):\n",
    "            for y in range(self.row_count):\n",
    "                if state[x][y] == self.EMPTY:\n",
    "                    empty += 1\n",
    "                    if empty >= self.column_count * self.row_count // 6:\n",
    "                        endgame = False\n",
    "                        break\n",
    "\n",
    "        black, white = self.count_influenced_territory_enhanced(state)\n",
    "                            \n",
    "        return black - (white + self.komi), endgame\n",
    "    \n",
    "    def count_influenced_territory_enhanced(self, board: list) -> tuple[int, int]:\n",
    "        '''\n",
    "        # Description \n",
    "        Calculates the territory influenced by black and white players on the Go board.\n",
    "\n",
    "        This function iterates through the board, analyzing each empty point to determine \n",
    "        if it's influenced by the surrounding black or white stones. The influence is calculated\n",
    "        based on the adjacent stones, with positive scores indicating black influence and negative\n",
    "        scores indicating white influence.\n",
    "\n",
    "        # Returns:\n",
    "        Tuple (black_territory, white_territory)\n",
    "        '''\n",
    "        black_territory = 0\n",
    "        white_territory = 0\n",
    "        visited = set()\n",
    "\n",
    "        # Function to calculate influence score\n",
    "        def influence_score(x, y):\n",
    "            score = 0\n",
    "            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < len(board) and 0 <= ny < len(board[0]):\n",
    "                    score += board[nx][ny]\n",
    "            return score\n",
    "\n",
    "        # Function to explore territory\n",
    "        def explore_territory(x, y):\n",
    "            nonlocal black_territory, white_territory\n",
    "            if (x, y) in visited or not (0 <= x < len(board) and 0 <= y < len(board[0])):\n",
    "                return\n",
    "            visited.add((x, y))\n",
    "\n",
    "            if board[x][y] == 0:\n",
    "                score = influence_score(x, y)\n",
    "                if score > 0:\n",
    "                    black_territory += 1\n",
    "                elif score < 0:\n",
    "                    white_territory += 1\n",
    "\n",
    "        for i in range(len(board)):\n",
    "            for j in range(len(board[0])):\n",
    "                if board[i][j] == 0 and (i, j) not in visited:\n",
    "                    explore_territory(i, j)\n",
    "\n",
    "        return black_territory, white_territory\n",
    "\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Changes Opponent\n",
    "        '''\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        '''\n",
    "        # Description\n",
    "        Returns the negative value of the value\n",
    "        '''\n",
    "        return -value\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        '''\n",
    "        # Description: \n",
    "        Converts the current state of the Go board into a 3-layer encoded format suitable for neural network input.\n",
    "        Each layer in the encoded format represents the presence of a specific type of stone or an empty space on the board:\n",
    "        - Layer 1 encodes the positions of white stones (represented by -1 in the input state) as 1s, and all other positions as 0s.\n",
    "        - Layer 2 encodes the positions of empty spaces (represented by 0 in the input state) as 1s, and all other positions as 0s.\n",
    "        - Layer 3 encodes the positions of black stones (represented by 1 in the input state) as 1s, and all other positions as 0s.\n",
    "        This encoding helps in clearly distinguishing between different elements on the board for machine learning applications.\n",
    "\n",
    "        # Returns: \n",
    "        A NumPy array of shape (3, height, width) containing the 3-layer encoded representation of the board state. Each layer is a 2D array where the board's height and width correspond to the dimensions of the original state.\n",
    "        '''\n",
    "        layer_1 = np.where(np.array(state) == -1, 1, 0).astype(np.float32)\n",
    "        layer_2 = np.where(np.array(state) == 0, 1, 0).astype(np.float32)\n",
    "        layer_3 = np.where(np.array(state) == 1, 1, 0).astype(np.float32)\n",
    "\n",
    "        result = np.stack([layer_1, layer_2, layer_3]).astype(np.float32)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        '''\n",
    "        # Description: \n",
    "        Adjusts the perspective of the Go board state based on the current player.\n",
    "\n",
    "        # Returns: \n",
    "        A two-dimensional array representing the Go board state adjusted for the current player's perspective.\n",
    "        '''\n",
    "        return state * player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Interface Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'player1':(201,153,255),\n",
    "      'player2':(179,236,255),\n",
    "      }\n",
    "\n",
    "SIZE_BOARD = 9\n",
    "BLACK = (0,0,0)\n",
    "WHITE = (255,255,255)\n",
    "GREEN = (140, 217, 166)\n",
    "\n",
    "SCREEN_SIZE=600\n",
    "SCREEN_PADDING = 50\n",
    "CELL_SIZE = (SCREEN_SIZE - SCREEN_PADDING) // SIZE_BOARD\n",
    "PIECE_SIZE = (SCREEN_SIZE - 2*SCREEN_PADDING) // SIZE_BOARD // 3\n",
    "\n",
    "screen=pygame.display.set_mode((SCREEN_SIZE,SCREEN_SIZE))\n",
    "\n",
    "pygame.display.set_caption(\"Go\")\n",
    "\n",
    "def to_pixels(x):\n",
    "    return SCREEN_PADDING + x*CELL_SIZE\n",
    "\n",
    "def to_coord(x):\n",
    "    quarter = CELL_SIZE//4\n",
    "    closest = (x-SCREEN_PADDING)//CELL_SIZE\n",
    "    if abs(to_pixels(closest)-(x-SCREEN_PADDING > to_pixels(closest)-(x-SCREEN_PADDING+quarter))):\n",
    "        closest = (x-SCREEN_PADDING+quarter)//CELL_SIZE\n",
    "    return closest\n",
    "\n",
    "def draw_board():\n",
    "    pygame.draw.rect(screen, GREEN, rect=(SCREEN_PADDING, SCREEN_PADDING, CELL_SIZE*(SIZE_BOARD-1), CELL_SIZE*(SIZE_BOARD-1)))\n",
    "    for i in range(SIZE_BOARD):\n",
    "        pygame.draw.line(screen, BLACK,(to_pixels(i),SCREEN_PADDING),(to_pixels(i),CELL_SIZE*(SIZE_BOARD-1) + SCREEN_PADDING),3)\n",
    "        pygame.draw.line(screen, BLACK,(SCREEN_PADDING,to_pixels(i)),(CELL_SIZE*(SIZE_BOARD-1)+SCREEN_PADDING,to_pixels(i)),3)\n",
    "\n",
    "def draw_piece(x,y,player):\n",
    "    color = BLACK if player == -1 else WHITE\n",
    "    pygame.draw.circle(screen,color,(to_pixels(x),to_pixels(y)),PIECE_SIZE)\n",
    "    pygame.draw.circle(screen,BLACK,(to_pixels(x),to_pixels(y)),PIECE_SIZE,3)\n",
    "\n",
    "def hover_to_select(player,valid_moves,click):\n",
    "    mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "    x, y = None, None\n",
    "    if ([to_coord(mouse_x), to_coord(mouse_y)] in valid_moves):\n",
    "        x, y = to_coord(mouse_x), to_coord(mouse_y)\n",
    "    \n",
    "    if (x!=None):\n",
    "        pixels = (to_pixels(x),to_pixels(y))\n",
    "        distance = pygame.math.Vector2(pixels[0] - mouse_x, pixels[1] - mouse_y).length()\n",
    "        if distance < PIECE_SIZE:\n",
    "            s = pygame.Surface((SCREEN_SIZE, SCREEN_SIZE), pygame.SRCALPHA)\n",
    "            if player == 1:\n",
    "                pygame.draw.circle(s,(255,255,255,200),(to_pixels(x),to_pixels(y)),PIECE_SIZE)\n",
    "            if player == -1:\n",
    "                pygame.draw.circle(s,(0,0,0,200),(to_pixels(x),to_pixels(y)),PIECE_SIZE)\n",
    "            pygame.draw.circle(s,BLACK,(to_pixels(x),to_pixels(y)),PIECE_SIZE,3)\n",
    "            screen.blit(s, (0, 0))\n",
    "        if click:\n",
    "            cur_pieces.append([x, y, player])\n",
    "            valid_moves.remove([x, y])\n",
    "            return [x, y, -1*player]\n",
    "    return [None, None, player]\n",
    "\n",
    "click = False\n",
    "valid_moves = []\n",
    "for i in range(SIZE_BOARD):\n",
    "    for j in range(SIZE_BOARD):\n",
    "        valid_moves.append([i, j])\n",
    "\n",
    "cur_pieces = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicar jogo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attaxx:\n",
    "    def __init__(self, args):\n",
    "        self.column_count = args[0]\n",
    "        self.row_count = args[1]\n",
    "        self.action_size = (self.column_count * self.row_count) ** 2 + 1\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        state = np.zeros((self.column_count, self.row_count))\n",
    "        state[0][0] = 1\n",
    "        state[self.column_count-1][self.row_count-1] = 1\n",
    "        state[0][self.column_count-1] = -1\n",
    "        state[self.row_count-1][0] = -1\n",
    "        return state\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        if action == self.action_size - 1:\n",
    "            return state\n",
    "        move = self.int_to_move(action)\n",
    "        a, b, a1, b1 = move[0], move[1], move[2], move[3]\n",
    "        if abs(a-a1)==2 or abs(b-b1)==2:\n",
    "            state[a][b] = 0\n",
    "            state[a1][b1] = player\n",
    "        else:\n",
    "            state[a1][b1] = player\n",
    "        self.capture_pieces(state, move, player)\n",
    "        return state\n",
    "\n",
    "    def is_valid_move(self, state, action, player):\n",
    "        a, b, a1, b1 = action\n",
    "        if (a==a1 and b==b1):\n",
    "            return False\n",
    "        if abs(a-a1)>2 or abs(b-b1)>2 or state[a1][b1]!=0 or state[a][b]!=player or ((abs(a-a1)==1 and abs(b-b1)==2) or (abs(a-a1)==2 and abs(b-b1)==1)):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def capture_pieces(self, state, action, player):\n",
    "        a, b, a1, b1 = action\n",
    "        for i in range(a1-1, a1+2):\n",
    "            for j in range(b1-1, b1+2):\n",
    "                try:\n",
    "                    if state[i][j]==-player and i>=0 and j>=0:\n",
    "                        state[i][j] = player\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "    def check_available_moves(self, state, player):\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                if state[i][j] == player:\n",
    "                    for a in range(self.column_count):\n",
    "                        for b in range(self.row_count):\n",
    "                            action = (i, j, a, b)\n",
    "                            if self.is_valid_move(state, action, player):\n",
    "                                return True\n",
    "        return False\n",
    "\n",
    "    def move_to_int(self, move):\n",
    "        return move[3] + move[2]*self.column_count + move[1]*self.column_count**2 + move[0]*self.column_count**3\n",
    "\n",
    "    def int_to_move(self, num):\n",
    "        move = [(num // self.column_count**3) % self.column_count, \n",
    "                (num // self.column_count**2) % self.column_count, \n",
    "                (num // self.column_count) % self.column_count, \n",
    "                num % self.column_count]\n",
    "        return move\n",
    "\n",
    "    \n",
    "    def get_valid_moves(self, state, player):\n",
    "        possible_moves = set()\n",
    "\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                state[i][j] = int(state[i][j])\n",
    "                if state[i][j] == player:\n",
    "                    moves_at_point = set(self.get_moves_at_point(state, player, i, j))\n",
    "                    possible_moves = possible_moves.union(moves_at_point)\n",
    "        \n",
    "        possible_moves_to_int = []\n",
    "        for move in possible_moves:\n",
    "            possible_moves_to_int.append(self.move_to_int(move))\n",
    "        binary_representation = [1 if i in possible_moves_to_int else 0 for i in range(self.action_size)]\n",
    "\n",
    "        return binary_representation\n",
    "\n",
    "    def get_moves_at_point(self, state, player, a, b):\n",
    "        moves_at_point = []\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                possible_action = (a, b, i, j)\n",
    "                if self.is_valid_move(state, possible_action, player):\n",
    "                    moves_at_point.append(possible_action)\n",
    "        return moves_at_point \n",
    "\n",
    "    def check_board_full(self, state):\n",
    "        for row in state:\n",
    "            if 0 in row:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def check_win_and_over(self, state, action):\n",
    "        # action não é necessário para o attaxx, mas é necessário para o go\n",
    "\n",
    "        count_player1 = 0\n",
    "        count_player2 = 0\n",
    "\n",
    "        for i in range(self.column_count):\n",
    "            for j in range(self.row_count):\n",
    "                if state[i][j] == 1:\n",
    "                    count_player1+=1\n",
    "                elif state[i][j] == -1:\n",
    "                    count_player2+=1\n",
    "        if count_player1 == 0:\n",
    "            return -1, True\n",
    "        elif count_player2 == 0:\n",
    "            return 1, True\n",
    "        \n",
    "        if self.check_board_full(state):\n",
    "            if count_player1>count_player2:\n",
    "                return 1, True\n",
    "            elif count_player2>count_player1:\n",
    "                return -1, True\n",
    "            elif count_player1==count_player2:\n",
    "                return 2, True\n",
    "        \n",
    "        return 0, False\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action, player):\n",
    "        winner, game_over = self.check_win_and_over(state, action = None)\n",
    "        return winner, game_over\n",
    "    \n",
    "    def print_board(self, state):\n",
    "        state = state.astype(int)\n",
    "        # Print column coordinates\n",
    "        print(\"   \", end=\"\")\n",
    "        for j in range(len(state[0])):\n",
    "            print(f\"{j:2}\", end=\" \")\n",
    "        print(\"\\n  +\", end=\"\")\n",
    "        for _ in range(len(state[0])):\n",
    "            print(\"---\", end=\"\")\n",
    "        print()\n",
    "\n",
    "        # Print rows with row coordinates\n",
    "        for i in range(len(state)):\n",
    "            print(f\"{i:2}|\", end=\" \")\n",
    "            for j in range(len(state[0])):\n",
    "                print(f\"{str(state[i][j]):2}\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        layer_1 = np.where(np.array(state) == -1, 1, 0).astype(np.float32) #returns same sized board replacing all -1 with 1 and all other positions with 0\n",
    "        layer_2 = np.where(np.array(state) == 0, 1, 0).astype(np.float32) #same logic for each possible number in position (-1, 1, or 0)\n",
    "        layer_3 = np.where(np.array(state) == 1, 1, 0).astype(np.float32)\n",
    "        \n",
    "        result = np.stack([layer_1, layer_2, layer_3]).astype(np.float32) #encoded state\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Interface Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UH OH WE STILL NEED THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    # Alpha Zero Node\n",
    "    ## Description:\n",
    "        A node for the AlphaZero MCTS. It contains the state, the action taken to get to the state, the prior probability of the action, the visit count, the value sum, and the children of the node.\n",
    "    ## Metohds:\n",
    "        - `is_expanded()`: Returns whether the node has been expanded.\n",
    "        - `select()`: Selects the best child node based on the UCB.\n",
    "        - `get_ucb()`: Returns the UCB of a child node.\n",
    "        - `expand()`: Expands the node by adding children.\n",
    "        - `backpropagate()`: Backpropagates the value of the node to the parent node.\n",
    "        '''\n",
    "    def __init__(self, game, args, state, player, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.player = player\n",
    "        self.prior = prior\n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_expanded(self):\n",
    "        '''\n",
    "        # is_expanded\n",
    "        ## Description:\n",
    "            Returns whether the node has been expanded.\n",
    "        ## Returns:\n",
    "            - `bool`: Whether the node has been expanded.'''\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        '''\n",
    "        # Description: \n",
    "        Selects the best child node from the current node's children in a Monte Carlo Tree Search using the Upper Confidence Bound (UCB) algorithm. \n",
    "\n",
    "        # Returns: \n",
    "        The best child node, chosen based on the highest UCB value or randomly if there's a tie.\n",
    "        '''\n",
    "        best_child = []\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = [child]\n",
    "                best_ucb = ucb\n",
    "            elif ucb == best_ucb:\n",
    "                best_child.append(child)\n",
    "                \n",
    "        return best_child[0] if len(best_child) == 1 else random.choice(best_child)\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        '''\n",
    "        # Description: \n",
    "        Calculates the Upper Confidence Bound (UCB) value for a given child node in a Monte Carlo Tree Search.\n",
    "\n",
    "        # Returns: \n",
    "        The calculated UCB value for the given child node.\n",
    "        '''\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        '''\n",
    "        # Description: \n",
    "        Expands the current node by adding new child nodes based on the given policy probabilities. For each possible action, it calculates the next state, adjusts the perspective for the opponent, and creates a new child node if the probability for that action is greater than zero.\n",
    "\n",
    "        # Returns: \n",
    "        None\n",
    "        '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "                child = Node(self.game, self.args, child_state, self.game.get_opponent(self.player), self, action, prob)\n",
    "                self.children.append(child)\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        '''\n",
    "        # Description: \n",
    "        Performs the backpropagation step in Monte Carlo Tree Search. It updates the current node's value sum and visit count based on the received value.\n",
    "\n",
    "        # Returns: \n",
    "        None\n",
    "        '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, game, args):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state, player):\n",
    "        \"\"\"\n",
    "        # Description:\n",
    "        Perform Monte Carlo Tree Search (MCTS) to find the best action.\n",
    "\n",
    "        # Returns:\n",
    "        An array of action probabilities for each possible action.\n",
    "        \"\"\"\n",
    "        root = Node(self.game, self.args, state, player, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state, player)\n",
    "\n",
    "        if self.args[\"game\"] == \"Attaxx\":\n",
    "            if np.sum(valid_moves) == 0:\n",
    "                valid_moves[-1] = 1\n",
    "            else:\n",
    "                valid_moves[-1] = 0\n",
    "\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_mcts_searches']):\n",
    "            node = root\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken, node.player)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if node.parent is not None:\n",
    "                if node.action_taken == self.game.action_size - 1 and node.parent.action_taken == self.game.action_size - 1 and self.args['game'] == 'Go':\n",
    "                    is_terminal = True # if the action is pass when the previous action was also pass, end the game\n",
    "\n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0))\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state, player)\n",
    "\n",
    "                if self.args[\"game\"] == \"Attaxx\":\n",
    "                    if np.sum(valid_moves) == 0:\n",
    "                        valid_moves[-1] = 1\n",
    "                    else:\n",
    "                        valid_moves[-1] = 0\n",
    "\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                node.expand(policy)\n",
    "\n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    # ResNet\n",
    "    ## Description:\n",
    "        A ResNet model for AlphaZero.\n",
    "        The model takes in a state and outputs a policy and value.\n",
    "         - The policy is a probability distribution over all possible actions.\n",
    "         - The value is a number between -1 and 1, where -1 means the current player loses and 1 means the current player wins following a tanh activation.\n",
    "        '''\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        # Description:\n",
    "        The forward pass of the model. This overrides the forward method of nn.Module so that it can be called directly on the model.\n",
    "\n",
    "        # Returns:\n",
    "        - `policy`: The policy output of the model.\n",
    "        - `value`: The value output of the model.\n",
    "        '''\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    '''\n",
    "    # Description:\n",
    "    A residual block for the ResNet model.\n",
    "    '''\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # Description:\n",
    "        Forward pass through the residual block.\n",
    "\n",
    "        # Returns:\n",
    "        Output tensor after passing through the block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(model, game, args)\n",
    "\n",
    "\n",
    "    def augment_state(self, state, probs):\n",
    "\n",
    "        augmented_states = []\n",
    "\n",
    "        skip_prob = probs[-1]\n",
    "        action_probs_matrix = np.array(probs[:-1]).reshape(self.game.column_count, self.game.row_count)\n",
    "        augmented_action_probs = []\n",
    "\n",
    "        def augment_and_append(transformed_state, transformed_probs_matrix):\n",
    "\n",
    "            # Append state\n",
    "            augmented_states.append(transformed_state)\n",
    "\n",
    "            # Flatten probs matrix, append the last value, and then append to augmented_action_probs\n",
    "            augmented_action_probs.append(list(transformed_probs_matrix.flatten()) + [skip_prob])\n",
    "\n",
    "        # Original state and probs\n",
    "        augment_and_append(state, action_probs_matrix)\n",
    "\n",
    "        # Rotate 90 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=1), np.rot90(action_probs_matrix, k=1))\n",
    "\n",
    "        # Rotate 180 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=2), np.rot90(action_probs_matrix, k=2))\n",
    "\n",
    "        # Rotate 270 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=3), np.rot90(action_probs_matrix, k=3))\n",
    "\n",
    "        # Flip horizontally\n",
    "        augment_and_append(np.fliplr(state), np.fliplr(action_probs_matrix))\n",
    "\n",
    "        # Flip vertically\n",
    "        augment_and_append(np.flipud(state), np.flipud(action_probs_matrix))\n",
    "\n",
    "        # Rotate 90 degrees clockwise and flip horizontally\n",
    "        augment_and_append(np.rot90(np.fliplr(state), k=1), np.rot90(np.fliplr(action_probs_matrix), k=1))\n",
    "\n",
    "        # Rotate 90 degrees clockwise and flip vertically\n",
    "        augment_and_append(np.rot90(np.flipud(state), k=1), np.rot90(np.flipud(action_probs_matrix), k=1))\n",
    "\n",
    "        return augmented_states, augmented_action_probs\n",
    "\n",
    "\n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        iter = 0\n",
    "        prev_skip = False\n",
    "\n",
    "        debugging = False\n",
    "\n",
    "        while True:\n",
    "            if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                print(\"\\nSEARCHING...\")\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(state, player)\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "            if self.args[\"game\"] == \"Go\":\n",
    "                print(f\"\\nPlayer: {player}\")\n",
    "                if action != self.game.action_size - 1:\n",
    "                    print(f\"Action: {action // self.game.row_count} {action % self.game.column_count}\")\n",
    "                else:\n",
    "                    print(f\"Action: Skip\")\n",
    "                \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                print(f\"Player: {player} with move {self.game.int_to_move(action)}\\nBoard:\")\n",
    "                self.game.print_board(state)    \n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action, player)\n",
    "\n",
    "            if self.args[\"game\"] == \"Go\":\n",
    "                self.game.print_board(state)\n",
    "                print(f\"Evaluation: {value}\")\n",
    "                b, w = self.game.count_influenced_territory_enhanced(state)\n",
    "                print(f\"Influence: B:{b} W: {w}\")\n",
    "                \n",
    "\n",
    "            if action == self.game.action_size - 1 and self.args['game'] == 'Go':\n",
    "                if prev_skip:\n",
    "                    is_terminal = True\n",
    "                else:\n",
    "                    prev_skip = True\n",
    "            else:\n",
    "                prev_skip = False\n",
    "\n",
    "            if is_terminal or iter >= self.args['max_moves']:\n",
    "                returnMemory = []\n",
    "\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "\n",
    "                    returnMemory.append((self.game.get_encoded_state(hist_neutral_state), hist_action_probs, hist_outcome))\n",
    "\n",
    "                return returnMemory\n",
    "\n",
    "            player = self.game.get_opponent(player)\n",
    "            iter += 1\n",
    "                \n",
    "    def train(self, memory):\n",
    "        '''\n",
    "        # Description:\n",
    "        Train the AlphaZero model using the provided memory data.\n",
    "\n",
    "        # Returns:\n",
    "        None\n",
    "        '''\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        '''\n",
    "        # Description:\n",
    "        Perform the AlphaZero learning process, including self-play, training, and model saving.\n",
    "\n",
    "        # Returns:\n",
    "        None\n",
    "        ''' \n",
    "        primary_memory = []\n",
    "\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            print(f\"Iteration {iteration + 1}\")\n",
    "\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                secondary_memory = self.selfPlay()\n",
    "\n",
    "            training_memory = []\n",
    "            if self.args['experience_replay']:\n",
    "                # training = secondary memory + 0.3 * old selfplay memories\n",
    "                primary_memory += secondary_memory\n",
    "                training_memory += secondary_memory\n",
    "\n",
    "                sample_size = int(len(primary_memory) * 0.3)\n",
    "                training_memory += random.sample(primary_memory, min(sample_size, len(primary_memory)))\n",
    "            else: \n",
    "                training_memory = secondary_memory\n",
    "\n",
    "            print(f\"Memory size: {len(training_memory)}\")\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(training_memory)\n",
    "\n",
    "            print(\"\\n\")\n",
    "                \n",
    "            torch.save(self.model.state_dict(), f\"AlphaZero/Models/{self.args['alias']}/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"AlphaZero/Models/{self.args['alias']}/optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 171\u001b[0m\n\u001b[0;32m    168\u001b[0m         a, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput your move: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 171\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_count\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m    172\u001b[0m     state \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mget_next_state(state, action, player)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "SAVE_NAME = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    GAME = input(\"Game: (Go/Attaxx) \")\n",
    "\n",
    "    LOAD = input(\"Load:\\nTrue will load a previous model, False will start from scratch (True/False):\\n\")\n",
    "\n",
    "    if LOAD == 'True':\n",
    "        LOAD = True\n",
    "        SAVE_NAME = input(\"Alias of the model: \")\n",
    "        MODEL = input(\"Model name: \")\n",
    "        OPT = input(\"Optimizer name: \")\n",
    "    else:\n",
    "        LOAD = False\n",
    "        SAVE_NAME = input(\"Alias of the new model: \")\n",
    "\n",
    "    TEST = input(\"Test:\\nTrue will play against the model, False will train the model (True/False):\\n\")\n",
    "\n",
    "\n",
    "    if TEST == 'True':\n",
    "        TEST = True\n",
    "        ONLINE = input(\"Test:\\nTrue will play online, False will play locally. (True/False):\\n\")\n",
    "        if ONLINE == 'True':\n",
    "            ONLINE = True\n",
    "        else:\n",
    "            ONLINE = False\n",
    "    else:\n",
    "        TEST = False\n",
    "\n",
    "    if GAME == 'Go':\n",
    "        size = int(input(\"Game size: (7/9) \"))\n",
    "        args = {\n",
    "            'game': 'Go',\n",
    "            'num_iterations': 20,             # number of highest level iterations\n",
    "            'num_selfPlay_iterations': 20,    # number of self-play games to play within each iteration\n",
    "            'num_mcts_searches': 500,         # number of mcts simulations when selecting a move within self-play\n",
    "            'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "            'num_epochs': 1200,               # number of epochs for training on self-play data for each iteration\n",
    "            'batch_size': 128,                # batch size for training\n",
    "            'temperature': 1.30,              # temperature for the softmax selection of moves\n",
    "            'C': 2,                           # the value of the constant policy\n",
    "            'experience_replay': True,        # recycle a certain % of old random selfplay data in the current training iteration\n",
    "            'augment': True,                  # whether to augment the training data with flipped and rotated states\n",
    "            'dirichlet_alpha': 0.03,          # the value of the dirichlet noise (alpha)\n",
    "            'dirichlet_epsilon': 0.25,        # the value of the dirichlet noise (epsilon)\n",
    "            'alias': ('Go' + SAVE_NAME)\n",
    "        }\n",
    "\n",
    "        game = Go(size = size, komi = 6.5)\n",
    "        model = ResNet(game, 9, 3, device)\n",
    "        optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    elif GAME == 'Attaxx':\n",
    "        size = int(input(\"Game size: (4/5/6) \"))\n",
    "        game_size = [size,size]\n",
    "        args = {\n",
    "            'game': 'Attaxx',\n",
    "            'num_iterations': 10,             # number of highest level iterations\n",
    "            'num_selfPlay_iterations': 1000,  # number of self-play games to play within each iteration\n",
    "            'num_mcts_searches': 500,         # number of mcts simulations when selecting a move within self-play\n",
    "            'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "            'num_epochs': 500,                # number of epochs for training on self-play data for each iteration\n",
    "            'batch_size': 500,                # batch size for training\n",
    "            'temperature': 1.25,              # temperature for the softmax selection of moves\n",
    "            'C': 2,                           # the value of the constant policy\n",
    "            'dirichlet_alpha': 0.3,           # the value of the dirichlet noise\n",
    "            'dirichlet_epsilon': 0.125,       # the value of the dirichlet noise\n",
    "            'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "            'augment': True,                  # whether to augment the training data with flipped and rotated states\n",
    "            'alias': ('Attaxx' + SAVE_NAME)\n",
    "        }\n",
    "\n",
    "        game = Attaxx(game_size)\n",
    "        model = ResNet(game, 20, 48, device)\n",
    "        optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    else:\n",
    "        print(\"Game Unavailable\")\n",
    "\n",
    "    if LOAD:\n",
    "        model.load_state_dict(torch.load(f'AlphaZero/Models/{GAME+SAVE_NAME}/{MODEL}.pt', map_location=device))\n",
    "        #model.load_state_dict(torch.load(f'AlphaZero/Models/{GAME+SAVE_NAME}/{MODEL}.pt', map_location=torch.device('cpu')))\n",
    "        optimizer.load_state_dict(torch.load(f'AlphaZero/Models/{GAME+SAVE_NAME}/{OPT}.pt', map_location=device))\n",
    "\n",
    "    if not TEST:\n",
    "        os.makedirs(f'AlphaZero/Models/{GAME+SAVE_NAME}', exist_ok=True)\n",
    "        alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "        alphaZero.learn()\n",
    "    else:\n",
    "\n",
    "        if not LOAD:\n",
    "            print(\"No model to test\")\n",
    "            exit()\n",
    "\n",
    "        if GAME == 'Go':\n",
    "\n",
    "            rendering = True\n",
    "\n",
    "            game = Go(size = size, komi= 6.5)\n",
    "            model.load_state_dict(torch.load(f'AlphaZero/Models/{GAME+SAVE_NAME}/{MODEL}.pt'))\n",
    "            mcts = MCTS(model, game, args)\n",
    "            state = game.get_initial_state()\n",
    "            player = 1\n",
    "\n",
    "            SIZE_BOARD = 9\n",
    "            BLACK = (0,0,0)\n",
    "            WHITE = (255,255,255)\n",
    "            GREEN = (140, 217, 166)\n",
    "            SCREEN_SIZE=600\n",
    "            SCREEN_PADDING = 50\n",
    "            CELL_SIZE = (SCREEN_SIZE - SCREEN_PADDING) // SIZE_BOARD\n",
    "            PIECE_SIZE = (SCREEN_SIZE - 2*SCREEN_PADDING) // SIZE_BOARD // 3\n",
    "\n",
    "            click = False\n",
    "            valid_moves = []\n",
    "            for i in range(SIZE_BOARD):\n",
    "                for j in range(SIZE_BOARD):\n",
    "                    valid_moves.append([i, j])\n",
    "\n",
    "            cur_pieces = []\n",
    "\n",
    "            if not rendering:\n",
    "                game.print_board(state)\n",
    "            else:\n",
    "                pygame.init()\n",
    "                pygame_icon = pygame.image.load('image.png')\n",
    "                pygame.display.set_icon(pygame_icon)\n",
    "\n",
    "                screen=pygame.display.set_mode((SCREEN_SIZE,SCREEN_SIZE))\n",
    "\n",
    "                pygame.display.set_caption(\"Go\")\n",
    "\n",
    "            while True:\n",
    "                \n",
    "                if rendering:\n",
    "                    for event in pygame.event.get():\n",
    "                        if event.type == pygame.QUIT:\n",
    "                            pygame.quit()\n",
    "                        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                            click = True\n",
    "                        if event.type == pygame.MOUSEBUTTONUP:\n",
    "                            click = False\n",
    "\n",
    "                    screen.fill(GREEN)\n",
    "                    draw_board()\n",
    "\n",
    "                    for piece in cur_pieces:\n",
    "                        if state[piece[0]][piece[1]] == 0:\n",
    "                            cur_pieces.remove([a, b, player])\n",
    "                            if game.is_valid_move(state, (a, b), player):\n",
    "                                valid_moves.append([a, b])\n",
    "                        else:\n",
    "                            draw_piece(piece[0], piece[1], piece[2])\n",
    "\n",
    "                if player == 1:\n",
    "\n",
    "                    if rendering:\n",
    "                        a, b, player = hover_to_select(player, valid_moves, click)\n",
    "                    else:\n",
    "                        a, b = tuple(int(x.strip()) for x in input(\"\\nInput your move: \").split(' '))\n",
    "                        print(\"\\n\")\n",
    "                    \n",
    "                    action = a * game.row_count + b\n",
    "                    state = game.get_next_state(state, action, player)\n",
    "\n",
    "                else:\n",
    "                    neut = game.change_perspective(state, player)\n",
    "                    action = mcts.search(neut, player)\n",
    "                    action = np.argmax(action)\n",
    "                    \n",
    "                    if rendering:\n",
    "                        a = action // game.row_count\n",
    "                        b = action % game.column_count\n",
    "\n",
    "                        cur_pieces.append([a, b, player])\n",
    "                        valid_moves.remove([a, b])\n",
    "                    else:\n",
    "                        print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "                    state = game.get_next_state(state, action, player)\n",
    "\n",
    "                winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                if win:\n",
    "                    if rendering:\n",
    "                        print(f\"player {winner} wins\")\n",
    "                    else:\n",
    "                        game.print_board(state)\n",
    "                        print(f\"player {winner} wins\")\n",
    "                        exit()\n",
    "\n",
    "                player = - player\n",
    "\n",
    "                if rendering:\n",
    "                    pygame.display.flip()\n",
    "                else:\n",
    "                    game.print_board(state)\n",
    "            \n",
    "        elif GAME == 'Attaxx':\n",
    "            \n",
    "            game = Attaxx([5,5])\n",
    "            model.load_state_dict(torch.load(f'AlphaZero/Models/{GAME+SAVE_NAME}/{MODEL}.pt', map_location=device))\n",
    "            mcts = MCTS(model, game, args)\n",
    "            state = game.get_initial_state()\n",
    "            game.print_board(state)\n",
    "\n",
    "            player = 1\n",
    "\n",
    "            while True:\n",
    "                if player == 1:\n",
    "                    move = tuple(int(x.strip()) for x in input(\"\\nInput your move: \").split(' '))\n",
    "                    print(\"\\n\")\n",
    "                    action = game.move_to_int(move)\n",
    "                    state = game.get_next_state(state, action, player)\n",
    "                else:\n",
    "                    action = mcts.search(state, player)\n",
    "                    action = np.argmax(action)\n",
    "                    print(f\"\\nAlphaZero Action: {game.int_to_move(action)}\\n\")\n",
    "                    state = game.get_next_state(state, action, player)\n",
    "\n",
    "                winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                if win:\n",
    "                    game.print_board(state)\n",
    "                    print(f\"player {winner} wins\")\n",
    "                    exit()\n",
    "\n",
    "                player = -player\n",
    "                \n",
    "                game.print_board(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Client Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ONLINE:\n",
    "    mode=\"A4x4\" # \"A6x6\" \"G7x7\" \"G9x9\" \"A5x5\"\n",
    "\n",
    "    def load_model():\n",
    "\n",
    "        global model, optimizer, mcts, game, args, state\n",
    "\n",
    "        if mode[0] == \"A\":\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 20,\n",
    "                'num_selfPlay_iterations': 150,\n",
    "                'num_mcts_searches': 100,\n",
    "                'max_moves': 512,\n",
    "                'num_epochs': 100,\n",
    "                'batch_size': 64,\n",
    "                'temperature': 1.25,\n",
    "                'C': 2,\n",
    "                'augment': False,\n",
    "                'dirichlet_alpha': 0.3,\n",
    "                'dirichlet_epsilon': 0.125,\n",
    "            }\n",
    "            game_size = int(mode[-1])\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 20, 48, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            state = game.get_initial_state()\n",
    "            if game_size == 4:\n",
    "                model.load_state_dict(torch.load(f'Models/a4x4.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'Models/a4x4.pt', map_location=device))\n",
    "                mcts = MCTS(model, game, args)\n",
    "            elif game_size == 6:\n",
    "                model.load_state_dict(torch.load(f'Models/a6x6.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'Models/a6x6.pt', map_location=device))\n",
    "\n",
    "        elif mode[0] == \"G\":\n",
    "            args = {\n",
    "                'game': 'Go',\n",
    "                'num_iterations': 20,\n",
    "                'num_selfPlay_iterations': 30,\n",
    "                'num_mcts_searches': 300,\n",
    "                'max_moves': 512,\n",
    "                'num_epochs': 400,\n",
    "                'batch_size': 128,\n",
    "                'temperature': 1.30,\n",
    "                'C': 2,\n",
    "                'augment': True,\n",
    "                'dirichlet_alpha': 0.03,\n",
    "                'dirichlet_epsilon': 0.25,\n",
    "            }\n",
    "\n",
    "            game_size = int(mode[-1])\n",
    "            game = Go(game_size)\n",
    "            state = game.get_initial_state()\n",
    "            model = ResNet(game, 9, 3, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "            if game_size == 7:\n",
    "                model.load_state_dict(torch.load(f'Models/g7x7.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'Models/g7x7.pt', map_location=device))\n",
    "                mcts = MCTS(model, game, args)\n",
    "            elif game_size == 8:\n",
    "                model.load_state_dict(torch.load(f'Models/g9x9.pt', map_location=device))\n",
    "                optimizer.load_state_dict(torch.load(f'Models/g9x9.pt', map_location=device))\n",
    "\n",
    "    def generate_move():\n",
    "        \n",
    "        if mode[-1] == \"A\":\n",
    "            action = mcts.search(state, player)\n",
    "            action = np.argmax(action)\n",
    "\n",
    "            move = game.int_to_move(action)\n",
    "            print(f\"\\nAlphaZero Action: {move}\\n\")\n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "            return f\"MOVE {move[0]} {move[1]} {move[2]} {move[3]}\"\n",
    "        \n",
    "        else:\n",
    "            action = mcts.search(state, player)\n",
    "            action = np.argmax(action)\n",
    "\n",
    "            print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "            return f\"MOVE {action // game.row_count} {action % game.column_count}\"\n",
    "\n",
    "    def apply_opponent_move(response):\n",
    "        numbers = [int(x) for x in response.split()[1:]]\n",
    "        if mode[-1] == \"A\":\n",
    "            action = game.move_to_int((response[0],response[1],response[2],response[3]))\n",
    "            state = game.get_next_state(state, action, player)\n",
    "        else:\n",
    "            action = numbers[0] * game.row_count + numbers[1]\n",
    "            state = game.get_next_state(state, action, player)\n",
    "\n",
    "    def connect_to_server(host='localhost', port=12345):\n",
    "        global player\n",
    "        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        client_socket.connect((host, port))\n",
    "        \n",
    "        response = client_socket.recv(1024).decode()\n",
    "        print(f\"Server ResponseINIT: {response}\")\n",
    "        \n",
    "        mode = response[-4:]\n",
    "        print(\"Playing:\", mode)\n",
    "        \n",
    "        if \"1\" in response:\n",
    "            player=1\n",
    "        else:\n",
    "            player=2\n",
    "            first=True\n",
    "        \n",
    "        while True:\n",
    "            # Generate and send a random move\n",
    "            \n",
    "            if player == 1 or not first:\n",
    "                move = generate_move()\n",
    "                time.sleep(1)\n",
    "                client_socket.send(move.encode())\n",
    "                print(\"Send:\",move)\n",
    "            \n",
    "                # Wait for server response\n",
    "                response = client_socket.recv(1024).decode()\n",
    "                apply_opponent_move(response)\n",
    "                print(f\"Server Response1: {response}\")\n",
    "                if \"END\" in response: break\n",
    "            \n",
    "            first=False\n",
    "            response = client_socket.recv(1024).decode()\n",
    "            print(f\"Server Response2: {response}\")\n",
    "            if \"END\" in response: break\n",
    "\n",
    "            # Add some condition to break the loop, if necessary\n",
    "            # Example: If server sends a certain messplayere, or after a number of moves\n",
    "\n",
    "        client_socket.close()\n",
    "\n",
    "    connect_to_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isia2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
